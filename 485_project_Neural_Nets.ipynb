{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "485 project - Neural Nets.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "3AvdUWKzCl42",
        "oNqADZ3qHJTb"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdInu_OFnnDH",
        "colab_type": "text"
      },
      "source": [
        "# Backpropagation learning for multilayer perceptrons"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MEGC2Vuuf-b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "aee8ce8f-9ed5-492c-da74-5e09566e90be"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "636ZmD9_vzGq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "5d670f6b-a948-482a-d3a0-1dbf94b176a8"
      },
      "source": [
        "import csv\n",
        "\n",
        "with open('sample_data/Weather.csv') as csv_file:\n",
        "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
        "    line_count = 0\n",
        "    for row in csv_reader:\n",
        "        if line_count == 0:\n",
        "            print(f'Column names are {\", \".join(row)}')\n",
        "            line_count += 1\n",
        "        else:\n",
        "            print(f'\\t In the year {row[0]}, the January temp was {row[1]}, and the February temp was {row[2]}.')\n",
        "            line_count += 1\n",
        "    print(f'Processed {line_count} lines.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-a384f9b3d42c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sample_data/Weather.csv'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcsv_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mcsv_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mline_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'sample_data/Weather.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0q5OMtAe0mYR",
        "colab_type": "text"
      },
      "source": [
        "#Processing Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AvdUWKzCl42",
        "colab_type": "text"
      },
      "source": [
        "##Reading in Information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLiQKDU50kyB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Need to represent cdc_week, disease, num_cases, month_avg_temp\n",
        "with open('/content/drive/Shared drives/COS485 project/influenza_test.csv') as csv_file:\n",
        "  csv_reader = csv.reader(csv_file, delimiter=',')\n",
        "  line_count = 0\n",
        "  for row in csv_reader:\n",
        "      if line_count == 0:\n",
        "          print(f'Column names are {\", \".join(row)}')\n",
        "          line_count += 1\n",
        "      elif line_count == 1:\n",
        "          print(f' Season: {row[0]}, \\n CDC Week: {row[1]} \\n Disease: {row[2]} \\n Number of Cases: {row[3]} \\n Average Temp of the Month: {row[4]} \\n')\n",
        "          line_count += 1\n",
        "  print(f'Processed {line_count} lines.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzUuCRNB-tqr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Need to represent cdc_week, disease, num_cases, month_avg_temp\n",
        "with open('/content/drive/Shared drives/COS485 project/influenza_train.csv') as csv_file:\n",
        "  csv_reader = csv.reader(csv_file, delimiter=',')\n",
        "  line_count = 0\n",
        "  for row in csv_reader:\n",
        "      if line_count == 0:\n",
        "          print(f'Column names are {\", \".join(row)}')\n",
        "          line_count += 1\n",
        "      elif line_count == 1:\n",
        "          print(f' Season: {row[0]}, \\n CDC Week: {row[1]} \\n Disease: {row[2]} \\n Number of Cases: {row[3]} \\n Average Temp of the Month: {row[4]} \\n')\n",
        "          line_count += 1\n",
        "  print(f'Processed {line_count} lines.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f79JpASEnnDM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# imports \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import numpy as np \n",
        "import scipy\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import matplotlib.gridspec as gridspec\n",
        "from IPython import display\n",
        "import time\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAi0-VQHnnDO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get data\n",
        "## mnist = torchvision.datasets.MNIST(root='data', train=True, download=True) # train data only\n",
        "## trainimages = mnist.train_data\n",
        "## trainlabels = mnist.train_labels\n",
        "\n",
        "# need some sort of structure(s) to hold train data and train labels\n",
        "# structure will hold all of the datapoints\n",
        "# each datapoint represents 1 week and has 3-4 facets plus the name of the week\n",
        "# 0) name: label/date of the week (or week #)\n",
        "# 1) number of new flu cases\n",
        "# 2) number of immunizations (if we can find that data)\n",
        "# 3) average temperature for the week\n",
        "# 4) tourist numbers/population density increase\n",
        "\n",
        "# Data structure\n",
        "# want to make a class for a datapoint\n",
        "# example: L = ['a', ['bb', ['ccc', 'ddd'], 'ee', 'ff'], 'g', 'h']\n",
        "\n",
        "# matrix of length 10 (we have 10 seasons of data) that holds matrices\n",
        "# those matrices are of length 32 (each season has 32 weeks)\n",
        "# each box of THOSE matrices are Data Structures holding the features data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_features(i):\n",
        "    \"\"\"return feature vector for i'th training example\"\"\"\n",
        "    im = trainimages[i].float() / 255.0 # rescale\n",
        "    ft = im.view(-1) # flatten\n",
        "    return ft \n",
        "\n",
        "def get_onehot(i):\n",
        "    \"\"\"return onehot label vector for i'th training example\"\"\"\n",
        "    onehot = torch.zeros(10).float()\n",
        "    onehot[trainlabels[i]] = 1.0\n",
        "    return onehot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lltLOCWNCxWZ",
        "colab_type": "text"
      },
      "source": [
        "#Running Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8US8UtbBlOn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Processing Dataset\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "def get_data():\n",
        "  train_data_path = '/content/drive/Shared drives/COS485 project/influenza_train.csv'\n",
        "  test_data_path = '/content/drive/Shared drives/COS485 project/influenza_test.csv'\n",
        "  \n",
        "  train = pd.read_csv(train_data_path)\n",
        "  test = pd.read_csv(test_data_path)\n",
        "\n",
        "  return train, test\n",
        "\n",
        "def get_combined_data():\n",
        "  #reading train data\n",
        "  train, test = get_data()\n",
        "\n",
        "  train_target = train.num_cases\n",
        "  train.drop(['num_cases'], axis=1, inplace = True)\n",
        "\n",
        "  test_target = test.num_cases\n",
        "  test.drop(['num_cases'], axis=1, inplace = True)\n",
        "\n",
        "  combined = train.append(test)\n",
        "  combined.reset_index(inplace=True)\n",
        "  combined.drop(['season', 'index'], inplace=True, axis=1)\n",
        "\n",
        "  return combined, train_target, test_target\n",
        "\n",
        "train_data, test_data = get_data()\n",
        "combined, train_target, test_target = get_combined_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltHM_zZLWcM1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "f570eb9f-b379-4773-e570-0215821eaa45"
      },
      "source": [
        "combined.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cdc_week</th>\n",
              "      <th>month_avg_temp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1029.000000</td>\n",
              "      <td>1029.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>16.620991</td>\n",
              "      <td>46.217784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>9.565369</td>\n",
              "      <td>10.710124</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>23.900000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>8.000000</td>\n",
              "      <td>37.700000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>16.000000</td>\n",
              "      <td>44.400000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>25.000000</td>\n",
              "      <td>55.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>33.000000</td>\n",
              "      <td>68.500000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          cdc_week  month_avg_temp\n",
              "count  1029.000000     1029.000000\n",
              "mean     16.620991       46.217784\n",
              "std       9.565369       10.710124\n",
              "min       1.000000       23.900000\n",
              "25%       8.000000       37.700000\n",
              "50%      16.000000       44.400000\n",
              "75%      25.000000       55.000000\n",
              "max      33.000000       68.500000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ss_5FbS7T0bA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_cols_with_no_nans(df,col_type):\n",
        "    '''\n",
        "    Arguments :\n",
        "    df : The dataframe to process\n",
        "    col_type : \n",
        "          num : to only get numerical columns with no nans\n",
        "          no_num : to only get nun-numerical columns with no nans\n",
        "          all : to get any columns with no nans    \n",
        "    '''\n",
        "    if (col_type == 'num'):\n",
        "        features = df.select_dtypes(exclude=['object'])\n",
        "    elif (col_type == 'no_num'):\n",
        "        features = df.select_dtypes(include=['object'])\n",
        "    elif (col_type == 'all'):\n",
        "        features = df\n",
        "    else :\n",
        "        print('Error : choose a type (num, no_num, all)')\n",
        "        return 0\n",
        "    cols_with_no_nans = []\n",
        "    for col in features.columns:\n",
        "        if not df[col].isnull().any():\n",
        "          cols_with_no_nans.append(col)\n",
        "    return cols_with_no_nans\n",
        "\n",
        "num_cols = get_cols_with_no_nans(combined , 'num')\n",
        "cat_cols = get_cols_with_no_nans(combined , 'no_num')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tmcd8uUVUtgt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "570dc816-79d4-4ff6-97cf-4ca37f3752b3"
      },
      "source": [
        "combined = combined[num_cols + cat_cols]\n",
        "combined.hist(figsize = (10, 5))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlkAAAE/CAYAAAB1vdadAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3de5hkdX3n8fcnDCiiKyCmd2TQISuaoBNRO2jWxO1ILigIJHFZWKKgJGPyYNR1Eh3Z7KNx15Vsgre4mh1FGW8gogYiqGEJjZpETFDicNEVcQgzDjMaBR1MNI3f/aPODEXTl5muPl2net6v5+mn69y/v1NVpz99zu9UpaqQJEnS4vqxYRcgSZK0HBmyJEmSWmDIkiRJaoEhS5IkqQWGLEmSpBYYsiRJklpgyJIk7bOSvDbJ+4ddh5YnQ5bmlGR1kkqyYti1zCbJhUn+x7DrkNRtSSaSbBl2HQvhcW40GbIkSZJaYMiSJA1dks1Jfj/Jl5Lck+SCJGNJPpHke0n+b5JDmnlPSnJTkruSTCb5qWnr+b1mPXcn+VCSByc5CPgE8KgkO5ufRzWLHZDkvc12bkoyvgf1rk/ytWaZm5P8ajP+QU1dT+yb95FJ/jnJjzfDr0yyLck3kvxmc7XgsXNsay1wBvDKpu6/aMY/KslHknwzydeTvLRvmdcm+XCS9zc1bkryuCSvTrIjyR1Jfrlv/skkb0jy+STfTXJZkkP36MnTrAxZ+7AkRyT5aPMG/ackb0uyX5I/SfKtJLcBJ0xb5tAk72kODt9J8ufzbOPaJL/ePH5GczA5oRk+LskNffO+KMktzXo/leQxfdN+MslVSb6d5CtJTp1lew9Lck2StybJALtH0tL7deCXgMcBz6UXis4FHknv79VLkzwOuAh4eTP+SuAvkhzQt55TgeOBI4GfBs6qqnuAZwPfqKqHNj/faOY/CbgYOBi4HHjbHtT6NeDngYcDfwi8P8nKqvoB8FHg9Gn1XFtVO5IcD7wC+EXgscDEfBuqqg3AB4D/1dT93CQ/BvwF8A/A4cBxwMuT/Erfos8F3gccAnwR+BS9/Xg48Drg/0zb1AuAFwErgSngrXuwHzQHQ9Y+Ksl+wMeB24HV9N50FwO/BZwIPBkYB543bdH3AQ8BngD8OPCmeTZ1LfcdRP4DcBvwzL7ha5t6TqZ3MP01egfOz9A7kNL8B3oV8MFmm6cBb09y9LQ2PQK4Gvjrqnpp+cWc0qj506raXlVb6R0DrquqL1bVvwAfo3dc+k/AFVV1VVX9K/AnwIHAv+9bz1ur6htV9W16QeSYebb72aq6sqrupXeMe9J8hVbVh5tt/KiqPgR8FTi2mfxBesepXf5zMw56ges9VXVTVX0feO1825rFzwCPrKrXVdUPq+o24J3TtvuZqvpUVU0BH6Z3bD2v2W8XA6uTHNw3//uq6sYmkP434NTmb4UWyJC17zoWeBTw+1V1T1X9S1V9lt4B4M1VdUdzgHrDrgWSrKT3n+BvV9V3qupfq+raebZzLb0wBb1w9Ya+4d0hC/ht4A1VdUtzQPifwDHN2awTgc1V9Z6qmqqqLwIfAf5j33Ye1azrw1X1BwvYH5KGb3vf43+eYfih9N7rt+8aWVU/Au6g94/iLnf2Pf5+s9xcps//4Mxzs0+SFyS5obk0eBfwROCwZvI1wEOSPC3Janoh72PNtEc19e7S/3hvPIbepc+7+mo4Fxjrm2f6/vtWEyR3DcP9901/LbcD+3Nfm7QAnb1jTK07Ari9CTT9ph8Abu97fATw7ar6zl5s52+BxyUZo3egOQn4wySH0Qt6n27mewzwliTn9y0begfOxwBPaw4iu6yg9x/nLicAO4E/24vaJI2ebwBrdg003QKOALbuwbKLcna7+efvnfQu0f1tVd3bdH0IQDN8Cb1LhtuBj1fV95rFtwGr+lZ3xB5udnrtdwBfr6qjFtiMmfTX8mjgX4FvLeL69zmeydp33QE8eob/1rbxwDda/zKHTju9PKfmdPj1wMuAG6vqh8Df0OuT8LWq2vUGvgN4cVUd3PdzYFX9TTPt2mnTHlpVv9O3qXcCnwSubC4vSlqeLgFOaPp07g+sA35A77gyn+3AI5I8fMAaDqIXer4JkOSF9M5k9fsgvUubZ3DfpULo1f/CJD+V5CH0Lsvtie3AT/QNfx74XpJXJTmw6U/7xCQ/s/fN2e03khzd1PU64NK+M19aAEPWvuvz9ALVeUkOSu/um2fQOwC8NMmq9O7kWb9rgaraRq8j6tuTHJJk/yTPnHHt93ct8BLuuzQ4OW0YemegXp3kCQBJHp5k1+XAj9M7G/b8Zpv7J/mZ9N1R1HgJ8BV6nWAP3OM9IWlkVNVXgN8A/pTeWZbnAs9t/oGbb9kv0+vreVtzie1R8y0zy3puBs6nd6Z+O70za389bZ7rgHvoXR34RN/4T9DrUH4NcCvwuWbSD+bZ7AXA0U3df96EnxPpXSH4Or198S56HfEX6n3AhfQunz4YeOmcc2tesW/wvivJo+m92X+e3n9lH6R3humP6d1l8l16nUrfBuxfVVPNLb1vonfnzgHANVX1a/Ns51fonWWaqKpr07u1eRNwWtNhdNd8zwdeSe/y4N3AVVX1omba44E30rvE+GP07qh5RVXdkORCYEtV/UFzx82FwL8FTmo6zEpSJzX/LN4IPGiG7htLWcck8P6qetewaliODFmSJC2h9D5T60p6d2pvBH5UVacMuaZJDFmLzsuFkiRNk+TRue9DS6f/PHr+NczpxcAOep+1dS/wO802b5ple2cMuD0NiWeyNLAk59K7dXi6z1TVs5e6HkmSusCQJUmS1AIvF0qSJLWgEx9Gethhh9Xq1at3D99zzz0cdFA3PuqoK7V0pQ7oTi1dqQO6U8uw67j++uu/VVWPHFoBI2D68a6Lhv06WgzLoQ2wPNqxHNoA92/HHh/rqmroP0996lOr3zXXXFNd0ZVaulJHVXdq6UodVd2pZdh1AH9fHTimdPln+vGui4b9OloMy6ENVcujHcuhDVX3b8eeHuu8XChJktQCQ5YkSVILDFmSJEktMGRJkiS1wJAlSZLUAkOWJElSCwxZkiRJLTBkSZIktcCQJUmS1AJDliRJUgsMWZIkSS3oxBdE743V669oZb2bzzuhlfVKkgazkOP+ujVTnDXPch731TbPZEmSJLXAkCVJktQCQ5YkSVILDFmSJEktMGRJkiS1wJAlSZLUAkOWJElSCwxZkiRJLTBkSZIktcCQJUmS1AJDliRJUgsMWZIkSS0wZEmSJLXAkCVJktQCQ5YkSVILDFmSJEktMGRJkiS1wJAlSZLUAkOWJElSC+YNWUnenWRHkhv7xv1xki8n+VKSjyU5uG/aq5PcmuQrSX6lrcIlSZK6bE/OZF0IHD9t3FXAE6vqp4H/B7waIMnRwGnAE5pl3p5kv0WrVpIkaUTMG7Kq6tPAt6eN+8uqmmoGPwesah6fDFxcVT+oqq8DtwLHLmK9kiRJI2HFIqzjRcCHmseH0wtdu2xpxj1AkrXAWoCxsTEmJyd3T9u5c+f9hvutWzM14/hBzba9uWpZSl2pAxZey6atdy9qHWMHwp9+4DLWHP7wRV3vQnTh+dm09e7d+2QxdWH/StIoGihkJfmvwBTwgb1dtqo2ABsAxsfHa2JiYve0yclJ+of7nbX+igVUOr/NZ8y8vblqWUpdqQMWXstiP3fr1kxx/qYVsz53S6kLz89Z66/YvU8WUxf2rySNogXfXZjkLOBE4Iyqqmb0VuCIvtlWNeMkqdOSHJHkmiQ3J7kpycua8YcmuSrJV5vfhzTjk+StzY0+X0rylOG2QFLXLChkJTkeeCVwUlV9v2/S5cBpSR6U5EjgKODzg5cpSa2bAtZV1dHA04Fzmpt51gNXV9VRwNXNMMCz6R3jjqLX9eEdS1+ypC7bk49wuAj4W+DxSbYkORt4G/Aw4KokNyT5M4Cqugm4BLgZ+CRwTlXd21r1krRIqmpbVX2hefw94BZ6fUpPBjY2s20ETmkenwy8t3o+BxycZOUSly2pw+btvFFVp88w+oI55n898PpBipKkYUqyGngycB0wVlXbmkl3AmPN48OBO/oW23WjzzYkicW5u1CSlo0kDwU+Ary8qr6bZPe0qqokNevCs69z1rupu6gLd8v2W8hd5WMHzr9cl9o4m649FwuxHNoAC2uHIUuSGkn2pxewPlBVH21Gb0+ysqq2NZcDdzTj9/hGn7nupu6iLtwt228hdybvyZ22o3DnbNeei4VYDm2AhbXD7y6UJHp3C9LrCnFLVb2xb9LlwJnN4zOBy/rGv6C5y/DpwN19lxUlyTNZktR4BvB8YFOSG5px5wLnAZc0N/3cDpzaTLsSeA69b7b4PvDCpS1XUtcZsiQJqKrPApll8nEzzF/AOa0WJWmkeblQkiSpBYYsSZKkFhiyJEmSWmDIkiRJaoEhS5IkqQWGLEmSpBYYsiRJklpgyJIkSWqBIUuSJKkFhixJkqQWGLIkSZJaYMiSJElqgSFLkiSpBYYsSZKkFhiyJEmSWmDIkiRJaoEhS5IkqQWGLEmSpBYYsiRJklpgyJIkSWqBIUuSJKkFhixJkqQWGLIkSZJaYMiSJElqgSFLkiSpBfOGrCTvTrIjyY194w5NclWSrza/D2nGJ8lbk9ya5EtJntJm8ZIkSV21J2eyLgSOnzZuPXB1VR0FXN0MAzwbOKr5WQu8Y3HKlCRJGi3zhqyq+jTw7WmjTwY2No83Aqf0jX9v9XwOODjJysUqVpIkaVQstE/WWFVtax7fCYw1jw8H7uibb0szTpIkaZ+yYtAVVFUlqb1dLslaepcUGRsbY3Jycve0nTt33m+437o1Uwuqcz6zbW+uWpZSV+qAhdey2M/d2IG9dXZhv3Th+Vm3Zmr3PllMw26XJI2qhYas7UlWVtW25nLgjmb8VuCIvvlWNeMeoKo2ABsAxsfHa2JiYve0yclJ+of7nbX+igWWPLfNZ8y8vblqWUpdqQMWXstiP3fr1kxx/qYVsz53S6kLz89Z66/YvU8WUxf2rySNooUejS8HzgTOa35f1jf+JUkuBp4G3N13WbHTVs8SANatmWot2O2Npapj83kntL4NSZL2BfOGrCQXARPAYUm2AK+hF64uSXI2cDtwajP7lcBzgFuB7wMvbKFmSZKkzps3ZFXV6bNMOm6GeQs4Z9CiJEmSRp2f+C5JktQCQ5YkSVILDFmSJEktMGRJkiS1wJAlSZLUAkOWJElSCwxZkiRJLTBkSZIktcCQJUmS1AJDliRJUgsMWZIkSS0wZEmSJLXAkCVJktQCQ5YkSVILVgy7AHXL6vVXzDvPujVTnLUH8y2VPam5bV3bJ1qYJO8GTgR2VNUTm3GvBX4L+GYz27lVdWUz7dXA2cC9wEur6lNLXrSkzvJMliTd50Lg+BnGv6mqjml+dgWso4HTgCc0y7w9yX5LVqmkzjNkSVKjqj4NfHsPZz8ZuLiqflBVXwduBY5trThJI8eQJUnze0mSLyV5d5JDmnGHA3f0zbOlGSdJgH2yJGk+7wD+O1DN7/OBF+3NCpKsBdYCjI2NMTk5ucglLq6dO3d2qsZ1a6b2epmxA+dfrkttnE3XnouFWA5tgIW1w5AlSXOoqu27Hid5J/DxZnArcETfrKuacTOtYwOwAWB8fLwmJiZaqXWxTE5O0qUaF3JTybo1U5y/ae4/cZvPmFhgRUuna8/FQiyHNsDC2uHlQkmaQ5KVfYO/CtzYPL4cOC3Jg5IcCRwFfH6p65PUXZ7JkqRGkouACeCwJFuA1wATSY6hd7lwM/BigKq6KcklwM3AFHBOVd07jLoldZMhS5IaVXX6DKMvmGP+1wOvb68iSaPMy4WSJEktMGRJkiS1wJAlSZLUAkOWJElSCwxZkiRJLTBkSZIktcCPcJAkaQSsXsAn3++Jzeed0Mp6NeCZrCT/JclNSW5MclGSByc5Msl1SW5N8qEkByxWsZIkSaNiwSEryeHAS4HxqnoisB9wGvBHwJuq6rHAd4CzF6NQSZKkUTJon6wVwIFJVgAPAbYBzwIubaZvBE4ZcBuSJEkjZ8Ehq6q2An8C/CO9cHU3cD1wV1VNNbNtAQ4ftEhJkqRRs+CO70kOAU4GjgTuAj4MHL8Xy68F1gKMjY0xOTm5e9rOnTvvN9xv3ZqpGce3ZezApd9ml+uA7tTSlTqgO7W0Ucds70VJ0twGubvwF4GvV9U3AZJ8FHgGcHCSFc3ZrFXA1pkWrqoNwAaA8fHxmpiY2D1tcnKS/uF+Z7V0d8Vs1q2Z4vxNw78Jsyt1QHdq6Uod0J1a2qhj8xkTi7o+SdpXDNIn6x+Bpyd5SJIAxwE3A9cAz2vmORO4bLASJUmSRs8gfbKuo9fB/QvApmZdG4BXAa9IcivwCOCCRahTkiRppAx0XaGqXgO8Ztro24BjB1mvJEnSqPNrdSRJklpgyJIkSWqBIUuSJKkFhixJkqQWGLIkSZJaYMiSJElqgSFLkiSpBYYsSZKkFhiyJEmSWmDIkiRJaoEhS5IkqQWGLEmSpBYYsiRJklpgyJIkSWqBIUuSJKkFhixJkqQWGLIkSZJaYMiSJElqgSFLkiSpBYYsSZKkFhiyJEmSWmDIkiRJaoEhS5IkqQWGLEmSpBasGHYBkiRpeFavv6KV9W4+74RW1jtKPJMlSZLUAkOWJElSCwxZkiRJLbBPliRpn2RfJLXNM1mSJEktGChkJTk4yaVJvpzkliQ/m+TQJFcl+Wrz+5DFKlaSJGlUDHq58C3AJ6vqeUkOAB4CnAtcXVXnJVkPrAdeNeB2JEmLZL7LZOvWTHHWAi6leZlMur8Fn8lK8nDgmcAFAFX1w6q6CzgZ2NjMthE4ZdAiJUmSRs0glwuPBL4JvCfJF5O8K8lBwFhVbWvmuRMYG7RISVoKSd6dZEeSG/vGzdgFIj1vTXJrki8lecrwKpfURYNcLlwBPAX43aq6Lslb6F0a3K2qKknNtHCStcBagLGxMSYnJ3dP27lz5/2G+61bMzVAyXtv7MCl32aX64Du1NKVOqA7tbRRx2zvxWXqQuBtwHv7xq1n5i4QzwaOan6eBryj+b3PautuPWlUDRKytgBbquq6ZvhSegef7UlWVtW2JCuBHTMtXFUbgA0A4+PjNTExsXva5OQk/cP9FtJPYBDr1kxx/qbhf9JFV+qA7tTSlTqgO7W0UcfmMyYWdX1dVlWfTrJ62uiTgYnm8UZgkl7IOhl4b1UV8LnmRqCVfWfyJe3jFny5sKruBO5I8vhm1HHAzcDlwJnNuDOBywaqUJKGa7YuEIcDd/TNt6UZJ0nA4HcX/i7wgebOwtuAF9ILbpckORu4HTh1wG1IUifM1QViLnN1jxiG+S4pd+Xy9yCG2YbFfH77u8+M2nOyq+65ugCNkoW0Y6CQVVU3AOMzTDpukPVKUofM1gViK3BE33yrmnEPMFf3iGGYr9tFVy5/D2KYbVjMS+z93WeWurvMoHbth7m6AI2ShbTDT3yXpLnN1gXicuAFzV2GTwfutj+WpH6j/a+KJC2iJBfR6+R+WJItwGuA85i5C8SVwHOAW4Hv0+suIUm7GbIkqVFVp88y6QFdIJq7Cs9ptyJJo8zLhZIkSS0wZEmSJLXAkCVJktQC+2RJkqRFt+trltatmVrUj5/YfN4Ji7autnkmS5IkqQWGLEmSpBYYsiRJklpgyJIkSWqBIUuSJKkFhixJkqQWGLIkSZJaYMiSJElqgSFLkiSpBX7iuyRJi2j1In66+WJ/WrqWlmeyJEmSWmDIkiRJaoEhS5IkqQWGLEmSpBYYsiRJklpgyJIkSWqBIUuSJKkFhixJkqQWGLIkSZJaYMiSJElqgSFLkiSpBYYsSZKkFhiyJEmSWmDIkiRJasHAISvJfkm+mOTjzfCRSa5LcmuSDyU5YPAyJUmSRstinMl6GXBL3/AfAW+qqscC3wHOXoRtSJIkjZSBQlaSVcAJwLua4QDPAi5tZtkInDLINiRJkkbRigGXfzPwSuBhzfAjgLuqaqoZ3gIcPtOCSdYCawHGxsaYnJzcPW3nzp33G+63bs3UjOPbMnbg0m+zy3VAd2rpSh3QnVraqGO296IkaW4LDllJTgR2VNX1SSb2dvmq2gBsABgfH6+JiftWMTk5Sf9wv7PWX7GAahdu3Zopzt80aBZdPnVAd2rpSh3QnVraqGPzGROLuj5J2lcMcjR+BnBSkucADwb+DfAW4OAkK5qzWauArYOXKUmSNFoW3Cerql5dVauqajVwGvBXVXUGcA3wvGa2M4HLBq5SkiRpxLTxOVmvAl6R5FZ6fbQuaGEbkiRJnbYonTeqahKYbB7fBhy7GOuVJEkaVX7iuyRJUgsMWZIkSS0wZEmSJLXAkCVJktQCQ5YkSVILDFmSJEktMGRJkiS1wJAlSZLUAkOWJElSCwxZkiRJLTBkSZIktWBRvrtQkpa7JJuB7wH3AlNVNZ7kUOBDwGpgM3BqVX1nWDVK6hbPZEnSnvuFqjqmqsab4fXA1VV1FHB1MyxJgCFLkgZxMrCxebwROGWItUjqGEOWJO2ZAv4yyfVJ1jbjxqpqW/P4TmBsOKVJ6iL7ZEnSnvm5qtqa5MeBq5J8uX9iVVWSmmnBJpStBRgbG2NycrL1Yueybs3UnNPHDpx/nq5bDm2A5dGOxW7DsN4/O3fu3OttG7IkaQ9U1dbm944kHwOOBbYnWVlV25KsBHbMsuwGYAPA+Ph4TUxMLFHVMztr/RVzTl+3ZorzN432n4fl0AZYHu1Y7DZsPmNi0da1NyYnJ9nb966XCyVpHkkOSvKwXY+BXwZuBC4HzmxmOxO4bDgVSuqi0Y7HkrQ0xoCPJYHecfODVfXJJH8HXJLkbOB24NQh1ijtE1bPcyZ2oTafd8Kir9OQJUnzqKrbgCfNMP6fgOOWviJJo8DLhZIkSS0wZEmSJLXAkCVJktQCQ5YkSVILDFmSJEktMGRJkiS1wJAlSZLUAkOWJElSCwxZkiRJLTBkSZIktWDBISvJEUmuSXJzkpuSvKwZf2iSq5J8tfl9yOKVK0mSNBoGOZM1BayrqqOBpwPnJDkaWA9cXVVHAVc3w5IkSfuUBYesqtpWVV9oHn8PuAU4HDgZ2NjMthE4ZdAiJUmSRs2i9MlKshp4MnAdMFZV25pJdwJji7ENSZKkUbJi0BUkeSjwEeDlVfXdJLunVVUlqVmWWwusBRgbG2NycnL3tJ07d95vuN+6NVODlrxXxg5c+m12uQ7oTi1dqQO6U0sbdcz2XpQkzW2gkJVkf3oB6wNV9dFm9PYkK6tqW5KVwI6Zlq2qDcAGgPHx8ZqYmNg9bXJykv7hfmetv2KQkvfaujVTnL9p4Cy6bOqA7tTSlTqgO7W0UcfmMyYWdX2StK8Y5O7CABcAt1TVG/smXQ6c2Tw+E7hs4eVJkiSNpkH+5X0G8HxgU5IbmnHnAucBlyQ5G7gdOHWwEiVJkkbPgkNWVX0WyCyTj1voeiVJkpYDP/FdkiSpBYYsSZKkFhiyJEmSWmDIkiRJaoEhS5IkqQWGLEmSpBYYsiRJklpgyJIkSWqBIUuSJKkFhixJkqQWGLIkSZJaYMiSJElqgSFLkiSpBYYsSZKkFhiyJEmSWmDIkiRJaoEhS5IkqQWGLEmSpBYYsiRJklpgyJIkSWrBimEXIEma2er1Vwy7BEkD8EyWJElSCwxZkiRJLTBkSZIktcCQJUmS1AJDliRJUgsMWZIkSS0wZEmSJLXAkCVJktQCQ5YkSVILWgtZSY5P8pUktyZZ39Z2JGmYPNZJmk0rISvJfsD/Bp4NHA2cnuToNrYlScPisU7SXNo6k3UscGtV3VZVPwQuBk5uaVuSNCwe6yTNqq2QdThwR9/wlmacJC0nHuskzSpVtfgrTZ4HHF9Vv9kMPx94WlW9pG+etcDaZvDxwFf6VnEY8K1FL2xhulJLV+qA7tTSlTqgO7UMu47HVNUjh7j9JbUnx7pm/FzHuy4a9utoMSyHNsDyaMdyaAPcvx17dKxb0VIhW4Ej+oZXNeN2q6oNwIaZFk7y91U13lJte6UrtXSlDuhOLV2pA7pTS1fq2IfMe6yDuY93XbQcXkfLoQ2wPNqxHNoAC2tHW5cL/w44KsmRSQ4ATgMub2lbkjQsHuskzaqVM1lVNZXkJcCngP2Ad1fVTW1sS5KGxWOdpLm0dbmQqroSuHKBi3fptHpXaulKHdCdWrpSB3Snlq7Usc8Y8FjXVcvhdbQc2gDLox3LoQ2wgHa00vFdkiRpX+fX6kiSJLWgcyGrK19RkWRzkk1Jbkjy90u87Xcn2ZHkxr5xhya5KslXm9+HDKmO1ybZ2uyXG5I8p+06mu0ekeSaJDcnuSnJy5rxS7pf5qhjyfdLkgcn+XySf2hq+cNm/JFJrmveQx9qOmRLD7CcXkNJ9kvyxSQfb4ZHsQ0P+LszjGP/oJIcnOTSJF9OckuSnx2ldiR5fN+x/IYk303y8oW0oVMhK937iopfqKpjhnDr6YXA8dPGrQeurqqjgKub4WHUAfCmZr8c0/RHWQpTwLqqOhp4OnBO89pY6v0yWx2w9PvlB8CzqupJwDHA8UmeDvxRU8tjge8AZy9BLRpNy+k19DLglr7hUWwDPPDvzjCO/YN6C/DJqvpJ4En0npeRaUdVfWXXsRx4KvB94GMsoA2dCln4FRUAVNWngW9PG30ysLF5vBE4ZUh1DEVVbauqLzSPv0fvTXs4S7xf5qhjyVXPzmZw/+angGcBlzbjl+S1otG0XF5DSVYBJwDvaobDiLVhDkt+7B9EkocDzwQuAKiqH1bVXYxYO/ocB3ytqm5nAW3oWsjq0ldUFPCXSa5P79Oah22sqrY1j+8ExoZYy0uSfKm5nLjkp3yTrAaeDFzHEPfLtDpgCPuluURyA7ADuAr4GnBXVU01s/g1L5rTMnkNvRl4JfCjZvgRjF4bYOa/O1069u+JI4FvAu9pLt++K8lBjF47djkNuKh5vNdt6FrI6pKfq6qn0Lt0eU6SZw67oF2qd0vosG4LfQfw7+hdWtgGnL+UG0/yUOAjwMur6rv905Zyv8xQx1D2S1Xd25zSXkXvTPBPLsV2tXyM+msoyVSMjNQAAAJASURBVInAjqq6fti1LII5/+4M+di/p1YATwHeUVVPBu5h2mW1EWkHTT++k4APT5+2p23oWsjao6+oWApVtbX5vYPetdhjh1FHn+1JVgI0v3cMo4iq2t4clH8EvJMl3C9J9qcXbD5QVR9tRi/5fpmpjmHul2b7dwHXAD8LHJxk12fgDe09pNEywq+hZwAnJdlMr4vJs+j1CRqlNgCz/t3pxLF/L2wBtlTVrjP8l9ILXaPWDuiF3S9U1fZmeK/b0LWQ1YmvqEhyUJKH7XoM/DJw49xLte5y4Mzm8ZnAZcMoYtcLrPGrLNF+afpYXADcUlVv7Ju0pPtltjqGsV+SPDLJwc3jA4FfotdH7Brgec1sQ3utqPuWw2uoql5dVauqajW9vxl/VVVnMEJtgDn/7nTi2L+nqupO4I4kj29GHQfczIi1o3E6910qhAW0oXMfRprere9v5r6vqHj9EGr4CXr/RUDv1OcHl7KOJBcBE/S+8Xs78Brgz4FLgEcDtwOnVlWrndJnqWOC3iWxAjYDL+67Rt1mLT8HfAbYxH39Ls6l1x9qyfbLHHWczhLvlyQ/Ta/z5X70/mG6pKpe17x+LwYOBb4I/EZV/aDNWjSalttrKMkE8HtVdeKotWG2vztJHsESH/sHleQYejchHADcBryQ5vXFiLSjCbr/CPxEVd3djNvr56JzIUuSJGk56NrlQkmSpGXBkCVJktQCQ5YkSVILDFmSJEktMGRJkiS1wJAlSZLUAkOWJElSCwxZkiRJLfj/vamOTWxOZ7wAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 720x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QVipzEgXNr6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "d28995d7-176a-43af-e177-b02d3bfb87ca"
      },
      "source": [
        "import seaborn as sb\n",
        "\n",
        "# Find correlation between features\n",
        "train_data = train_data[num_cols + cat_cols]\n",
        "train_data['Target'] = train_target\n",
        "\n",
        "C_mat = train_data.corr()\n",
        "fig = plt.figure(figsize = (5,5))\n",
        "\n",
        "sb.heatmap(C_mat, vmax = .8, square = True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjOGq1Coacm2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "2e00b1b1-2b7b-4cc8-c331-90305e8d33f1"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def oneHotEncode(df,colNames):\n",
        "    for col in colNames:\n",
        "        if( df[col].dtype == np.dtype('object')):\n",
        "            dummies = pd.get_dummies(df[col],prefix=col)\n",
        "            df = pd.concat([df,dummies],axis=1)\n",
        "\n",
        "            #drop the encoded column\n",
        "            df.drop([col],axis = 1 , inplace=True)\n",
        "    return df\n",
        "    \n",
        "\n",
        "print('There were {} columns before encoding categorical features'.format(combined.shape[1]))\n",
        "combined = oneHotEncode(combined, cat_cols)\n",
        "print('There are {} columns after encoding categorical features'.format(combined.shape[1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There were 3 columns before encoding categorical features\n",
            "There are 5 columns after encoding categorical features\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1clM5jJawFH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "5802f91b-806e-41aa-80c4-ce726cb1c25d"
      },
      "source": [
        "combined.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cdc_week</th>\n",
              "      <th>month_avg_temp</th>\n",
              "      <th>disease_INFLUENZA_A</th>\n",
              "      <th>disease_INFLUENZA_B</th>\n",
              "      <th>disease_INFLUENZA_UNSPECIFIED</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1029.000000</td>\n",
              "      <td>1029.000000</td>\n",
              "      <td>1029.000000</td>\n",
              "      <td>1029.000000</td>\n",
              "      <td>1029.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>16.620991</td>\n",
              "      <td>46.217784</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>9.565369</td>\n",
              "      <td>10.710124</td>\n",
              "      <td>0.471634</td>\n",
              "      <td>0.471634</td>\n",
              "      <td>0.471634</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>23.900000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>8.000000</td>\n",
              "      <td>37.700000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>16.000000</td>\n",
              "      <td>44.400000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>25.000000</td>\n",
              "      <td>55.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>33.000000</td>\n",
              "      <td>68.500000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          cdc_week  ...  disease_INFLUENZA_UNSPECIFIED\n",
              "count  1029.000000  ...                    1029.000000\n",
              "mean     16.620991  ...                       0.333333\n",
              "std       9.565369  ...                       0.471634\n",
              "min       1.000000  ...                       0.000000\n",
              "25%       8.000000  ...                       0.000000\n",
              "50%      16.000000  ...                       0.000000\n",
              "75%      25.000000  ...                       1.000000\n",
              "max      33.000000  ...                       1.000000\n",
              "\n",
              "[8 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cz9MKQmbICo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "57fab6f3-d1a0-4fc9-e0be-18839ad7804f"
      },
      "source": [
        "print(len(train_data))\n",
        "print(len(test_data))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "792\n",
            "237\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSuF4NQwa7-9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_combined():\n",
        "    global combined\n",
        "    num_train_ex = len(train_data)\n",
        "    train = combined[:num_train_ex]\n",
        "    test = combined[num_train_ex:]\n",
        "\n",
        "    return train , test \n",
        "  \n",
        "train, test = split_combined()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCk-WAutFVik",
        "colab_type": "text"
      },
      "source": [
        "#Making Network: 1 Layer Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORH0farIFUqa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "13538110-6253-49b9-9fe5-76ffd0f2860a"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error \n",
        "\n",
        "NN_model_1 = Sequential()\n",
        "\n",
        "#Input Layer:\n",
        "NN_model_1.add(Dense(128, kernel_initializer='normal', input_dim = train.shape[1], activation='relu'))\n",
        "\n",
        "#Single Hidden Layer:\n",
        "NN_model_1.add(Dense(256, kernel_initializer='normal', activation='relu'))\n",
        "\n",
        "# The output layer:\n",
        "NN_model_1.add(Dense(1, kernel_initializer='normal', activation='linear'))\n",
        "\n",
        "# Compile the network :\n",
        "NN_model_1.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
        "NN_model_1.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 128)               768       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 256)               33024     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 34,049\n",
            "Trainable params: 34,049\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkKn-9hKGz6s",
        "colab_type": "text"
      },
      "source": [
        "##Checkpoint Callback"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aj-RLWYnGzMK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \n",
        "checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swGcRJlmG7_x",
        "colab_type": "text"
      },
      "source": [
        "## Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DTmfllNG-eX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "379f619f-a904-485c-a7b0-70497b05232c"
      },
      "source": [
        "NN_model_1.fit(train, train_target, epochs=50, batch_size=32, validation_split = 0.2, callbacks=callbacks_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 633 samples, validate on 159 samples\n",
            "Epoch 1/50\n",
            "633/633 [==============================] - 0s 296us/step - loss: 14.0238 - mean_absolute_error: 14.0238 - val_loss: 33.8525 - val_mean_absolute_error: 33.8525\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 33.85254, saving model to Weights-001--33.85254.hdf5\n",
            "Epoch 2/50\n",
            "633/633 [==============================] - 0s 61us/step - loss: 13.7297 - mean_absolute_error: 13.7297 - val_loss: 32.6439 - val_mean_absolute_error: 32.6439\n",
            "\n",
            "Epoch 00002: val_loss improved from 33.85254 to 32.64394, saving model to Weights-002--32.64394.hdf5\n",
            "Epoch 3/50\n",
            "633/633 [==============================] - 0s 58us/step - loss: 13.6909 - mean_absolute_error: 13.6909 - val_loss: 33.0655 - val_mean_absolute_error: 33.0655\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 32.64394\n",
            "Epoch 4/50\n",
            "633/633 [==============================] - 0s 67us/step - loss: 13.6454 - mean_absolute_error: 13.6454 - val_loss: 32.8770 - val_mean_absolute_error: 32.8770\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 32.64394\n",
            "Epoch 5/50\n",
            "633/633 [==============================] - 0s 62us/step - loss: 13.5347 - mean_absolute_error: 13.5347 - val_loss: 32.7240 - val_mean_absolute_error: 32.7240\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 32.64394\n",
            "Epoch 6/50\n",
            "633/633 [==============================] - 0s 72us/step - loss: 13.4379 - mean_absolute_error: 13.4378 - val_loss: 32.6922 - val_mean_absolute_error: 32.6922\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 32.64394\n",
            "Epoch 7/50\n",
            "633/633 [==============================] - 0s 63us/step - loss: 13.3340 - mean_absolute_error: 13.3340 - val_loss: 32.6854 - val_mean_absolute_error: 32.6854\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 32.64394\n",
            "Epoch 8/50\n",
            "633/633 [==============================] - 0s 61us/step - loss: 13.2400 - mean_absolute_error: 13.2400 - val_loss: 32.4349 - val_mean_absolute_error: 32.4349\n",
            "\n",
            "Epoch 00008: val_loss improved from 32.64394 to 32.43495, saving model to Weights-008--32.43495.hdf5\n",
            "Epoch 9/50\n",
            "633/633 [==============================] - 0s 66us/step - loss: 13.0459 - mean_absolute_error: 13.0459 - val_loss: 33.0400 - val_mean_absolute_error: 33.0400\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 32.43495\n",
            "Epoch 10/50\n",
            "633/633 [==============================] - 0s 61us/step - loss: 12.9811 - mean_absolute_error: 12.9811 - val_loss: 32.8263 - val_mean_absolute_error: 32.8263\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 32.43495\n",
            "Epoch 11/50\n",
            "633/633 [==============================] - 0s 62us/step - loss: 12.8024 - mean_absolute_error: 12.8025 - val_loss: 31.6004 - val_mean_absolute_error: 31.6004\n",
            "\n",
            "Epoch 00011: val_loss improved from 32.43495 to 31.60039, saving model to Weights-011--31.60039.hdf5\n",
            "Epoch 12/50\n",
            "633/633 [==============================] - 0s 62us/step - loss: 12.7867 - mean_absolute_error: 12.7867 - val_loss: 32.1531 - val_mean_absolute_error: 32.1531\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 31.60039\n",
            "Epoch 13/50\n",
            "633/633 [==============================] - 0s 65us/step - loss: 12.6205 - mean_absolute_error: 12.6205 - val_loss: 31.4204 - val_mean_absolute_error: 31.4204\n",
            "\n",
            "Epoch 00013: val_loss improved from 31.60039 to 31.42042, saving model to Weights-013--31.42042.hdf5\n",
            "Epoch 14/50\n",
            "633/633 [==============================] - 0s 62us/step - loss: 12.5484 - mean_absolute_error: 12.5484 - val_loss: 32.6805 - val_mean_absolute_error: 32.6805\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 31.42042\n",
            "Epoch 15/50\n",
            "633/633 [==============================] - 0s 63us/step - loss: 12.6164 - mean_absolute_error: 12.6164 - val_loss: 31.2585 - val_mean_absolute_error: 31.2585\n",
            "\n",
            "Epoch 00015: val_loss improved from 31.42042 to 31.25849, saving model to Weights-015--31.25849.hdf5\n",
            "Epoch 16/50\n",
            "633/633 [==============================] - 0s 58us/step - loss: 12.6673 - mean_absolute_error: 12.6673 - val_loss: 32.4357 - val_mean_absolute_error: 32.4357\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 31.25849\n",
            "Epoch 17/50\n",
            "633/633 [==============================] - 0s 67us/step - loss: 12.5206 - mean_absolute_error: 12.5206 - val_loss: 31.9868 - val_mean_absolute_error: 31.9868\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 31.25849\n",
            "Epoch 18/50\n",
            "633/633 [==============================] - 0s 66us/step - loss: 12.4087 - mean_absolute_error: 12.4087 - val_loss: 31.0514 - val_mean_absolute_error: 31.0514\n",
            "\n",
            "Epoch 00018: val_loss improved from 31.25849 to 31.05139, saving model to Weights-018--31.05139.hdf5\n",
            "Epoch 19/50\n",
            "633/633 [==============================] - 0s 70us/step - loss: 12.5305 - mean_absolute_error: 12.5305 - val_loss: 31.8514 - val_mean_absolute_error: 31.8514\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 31.05139\n",
            "Epoch 20/50\n",
            "633/633 [==============================] - 0s 62us/step - loss: 12.3256 - mean_absolute_error: 12.3256 - val_loss: 32.5570 - val_mean_absolute_error: 32.5570\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 31.05139\n",
            "Epoch 21/50\n",
            "633/633 [==============================] - 0s 61us/step - loss: 12.3867 - mean_absolute_error: 12.3867 - val_loss: 31.0426 - val_mean_absolute_error: 31.0426\n",
            "\n",
            "Epoch 00021: val_loss improved from 31.05139 to 31.04256, saving model to Weights-021--31.04256.hdf5\n",
            "Epoch 22/50\n",
            "633/633 [==============================] - 0s 73us/step - loss: 12.2609 - mean_absolute_error: 12.2609 - val_loss: 33.0575 - val_mean_absolute_error: 33.0575\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 31.04256\n",
            "Epoch 23/50\n",
            "633/633 [==============================] - 0s 69us/step - loss: 12.3174 - mean_absolute_error: 12.3174 - val_loss: 30.2530 - val_mean_absolute_error: 30.2530\n",
            "\n",
            "Epoch 00023: val_loss improved from 31.04256 to 30.25297, saving model to Weights-023--30.25297.hdf5\n",
            "Epoch 24/50\n",
            "633/633 [==============================] - 0s 58us/step - loss: 12.3197 - mean_absolute_error: 12.3197 - val_loss: 31.5590 - val_mean_absolute_error: 31.5590\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 30.25297\n",
            "Epoch 25/50\n",
            "633/633 [==============================] - 0s 62us/step - loss: 12.1913 - mean_absolute_error: 12.1913 - val_loss: 31.4855 - val_mean_absolute_error: 31.4855\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 30.25297\n",
            "Epoch 26/50\n",
            "633/633 [==============================] - 0s 62us/step - loss: 12.1541 - mean_absolute_error: 12.1541 - val_loss: 32.1525 - val_mean_absolute_error: 32.1525\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 30.25297\n",
            "Epoch 27/50\n",
            "633/633 [==============================] - 0s 70us/step - loss: 12.2264 - mean_absolute_error: 12.2264 - val_loss: 30.5403 - val_mean_absolute_error: 30.5403\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 30.25297\n",
            "Epoch 28/50\n",
            "633/633 [==============================] - 0s 58us/step - loss: 12.1517 - mean_absolute_error: 12.1517 - val_loss: 31.2690 - val_mean_absolute_error: 31.2690\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 30.25297\n",
            "Epoch 29/50\n",
            "633/633 [==============================] - 0s 62us/step - loss: 11.9983 - mean_absolute_error: 11.9983 - val_loss: 31.8983 - val_mean_absolute_error: 31.8983\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 30.25297\n",
            "Epoch 30/50\n",
            "633/633 [==============================] - 0s 60us/step - loss: 12.0333 - mean_absolute_error: 12.0333 - val_loss: 30.5772 - val_mean_absolute_error: 30.5772\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 30.25297\n",
            "Epoch 31/50\n",
            "633/633 [==============================] - 0s 64us/step - loss: 12.0054 - mean_absolute_error: 12.0054 - val_loss: 32.7455 - val_mean_absolute_error: 32.7455\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 30.25297\n",
            "Epoch 32/50\n",
            "633/633 [==============================] - 0s 66us/step - loss: 11.9882 - mean_absolute_error: 11.9882 - val_loss: 30.5527 - val_mean_absolute_error: 30.5527\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 30.25297\n",
            "Epoch 33/50\n",
            "633/633 [==============================] - 0s 61us/step - loss: 11.8566 - mean_absolute_error: 11.8566 - val_loss: 31.9938 - val_mean_absolute_error: 31.9938\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 30.25297\n",
            "Epoch 34/50\n",
            "633/633 [==============================] - 0s 63us/step - loss: 11.8387 - mean_absolute_error: 11.8387 - val_loss: 30.4687 - val_mean_absolute_error: 30.4687\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 30.25297\n",
            "Epoch 35/50\n",
            "633/633 [==============================] - 0s 59us/step - loss: 11.8518 - mean_absolute_error: 11.8518 - val_loss: 32.2220 - val_mean_absolute_error: 32.2220\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 30.25297\n",
            "Epoch 36/50\n",
            "633/633 [==============================] - 0s 62us/step - loss: 11.8123 - mean_absolute_error: 11.8123 - val_loss: 30.7364 - val_mean_absolute_error: 30.7364\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 30.25297\n",
            "Epoch 37/50\n",
            "633/633 [==============================] - 0s 64us/step - loss: 11.6375 - mean_absolute_error: 11.6375 - val_loss: 31.8046 - val_mean_absolute_error: 31.8046\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 30.25297\n",
            "Epoch 38/50\n",
            "633/633 [==============================] - 0s 60us/step - loss: 11.6402 - mean_absolute_error: 11.6402 - val_loss: 30.3196 - val_mean_absolute_error: 30.3196\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 30.25297\n",
            "Epoch 39/50\n",
            "633/633 [==============================] - 0s 61us/step - loss: 11.6183 - mean_absolute_error: 11.6183 - val_loss: 30.8995 - val_mean_absolute_error: 30.8995\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 30.25297\n",
            "Epoch 40/50\n",
            "633/633 [==============================] - 0s 65us/step - loss: 11.5514 - mean_absolute_error: 11.5514 - val_loss: 30.0552 - val_mean_absolute_error: 30.0552\n",
            "\n",
            "Epoch 00040: val_loss improved from 30.25297 to 30.05523, saving model to Weights-040--30.05523.hdf5\n",
            "Epoch 41/50\n",
            "633/633 [==============================] - 0s 59us/step - loss: 11.4430 - mean_absolute_error: 11.4430 - val_loss: 30.2000 - val_mean_absolute_error: 30.2000\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 30.05523\n",
            "Epoch 42/50\n",
            "633/633 [==============================] - 0s 65us/step - loss: 11.4758 - mean_absolute_error: 11.4758 - val_loss: 30.6177 - val_mean_absolute_error: 30.6177\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 30.05523\n",
            "Epoch 43/50\n",
            "633/633 [==============================] - 0s 63us/step - loss: 11.3723 - mean_absolute_error: 11.3723 - val_loss: 31.6436 - val_mean_absolute_error: 31.6436\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 30.05523\n",
            "Epoch 44/50\n",
            "633/633 [==============================] - 0s 61us/step - loss: 11.3262 - mean_absolute_error: 11.3262 - val_loss: 30.1923 - val_mean_absolute_error: 30.1923\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 30.05523\n",
            "Epoch 45/50\n",
            "633/633 [==============================] - 0s 63us/step - loss: 11.2458 - mean_absolute_error: 11.2458 - val_loss: 32.0569 - val_mean_absolute_error: 32.0569\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 30.05523\n",
            "Epoch 46/50\n",
            "633/633 [==============================] - 0s 64us/step - loss: 11.2871 - mean_absolute_error: 11.2871 - val_loss: 31.9026 - val_mean_absolute_error: 31.9026\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 30.05523\n",
            "Epoch 47/50\n",
            "633/633 [==============================] - 0s 74us/step - loss: 11.3756 - mean_absolute_error: 11.3756 - val_loss: 32.5595 - val_mean_absolute_error: 32.5595\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 30.05523\n",
            "Epoch 48/50\n",
            "633/633 [==============================] - 0s 66us/step - loss: 11.3179 - mean_absolute_error: 11.3179 - val_loss: 31.7177 - val_mean_absolute_error: 31.7177\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 30.05523\n",
            "Epoch 49/50\n",
            "633/633 [==============================] - 0s 64us/step - loss: 11.2031 - mean_absolute_error: 11.2031 - val_loss: 31.0654 - val_mean_absolute_error: 31.0654\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 30.05523\n",
            "Epoch 50/50\n",
            "633/633 [==============================] - 0s 61us/step - loss: 11.1446 - mean_absolute_error: 11.1446 - val_loss: 31.1689 - val_mean_absolute_error: 31.1689\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 30.05523\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f72e1e11898>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79C8o9Qyd_03",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load weights file of the best model\n",
        "# weights_file = 'Weights-036--29.68963.hdf5' # choose the best checkpoint \n",
        "weights_file = '/content/drive/Shared drives/COS485 project/Weights/Weights-036--29.68963.hdf5'\n",
        "NN_model_1.load_weights(weights_file) # load it\n",
        "NN_model_1.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enT3Per1fT5J",
        "colab_type": "text"
      },
      "source": [
        "The validation loss of the best model is 29.19424\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmqesmoHmSmx",
        "colab_type": "text"
      },
      "source": [
        "#Network 2: 2 Layers, Sigmoid and Relu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sJzRh6iemt5Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "outputId": "81f0948e-94b1-4dfd-db22-bdeb40197494"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error \n",
        "\n",
        "NN_model_2 = Sequential()\n",
        "\n",
        "#Input Layer:\n",
        "NN_model_2.add(Dense(128, kernel_initializer='normal', input_dim = train.shape[1], activation='relu'))\n",
        "\n",
        "#Two Hidden Layers:\n",
        "NN_model_2.add(Dense(256, kernel_initializer='normal', activation='sigmoid'))\n",
        "NN_model_2.add(Dense(256, kernel_initializer='normal', activation='relu'))\n",
        "\n",
        "# The output layer:\n",
        "NN_model_2.add(Dense(1, kernel_initializer='normal', activation='linear'))\n",
        "\n",
        "# Compile the network :\n",
        "NN_model_2.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
        "NN_model_2.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_4 (Dense)              (None, 128)               768       \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 256)               33024     \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 99,841\n",
            "Trainable params: 99,841\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMh3h06Mm36W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_name = 'Weights-{epoch:04d}--{val_loss:.5f}.hdf5' \n",
        "checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HR-jHj0_m8Fs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0193151a-c497-4103-ed94-8b13c25238f9"
      },
      "source": [
        "NN_model_2.fit(train, train_target, epochs=50, batch_size=32, validation_split = 0.2, callbacks=callbacks_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 633 samples, validate on 159 samples\n",
            "Epoch 1/50\n",
            "633/633 [==============================] - 0s 241us/step - loss: 13.9605 - mean_absolute_error: 13.9605 - val_loss: 33.8764 - val_mean_absolute_error: 33.8764\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 33.87637, saving model to Weights-001--33.87637.hdf5\n",
            "Epoch 2/50\n",
            "633/633 [==============================] - 0s 84us/step - loss: 13.7442 - mean_absolute_error: 13.7442 - val_loss: 33.2643 - val_mean_absolute_error: 33.2643\n",
            "\n",
            "Epoch 00002: val_loss improved from 33.87637 to 33.26434, saving model to Weights-002--33.26434.hdf5\n",
            "Epoch 3/50\n",
            "633/633 [==============================] - 0s 85us/step - loss: 13.5512 - mean_absolute_error: 13.5512 - val_loss: 32.6962 - val_mean_absolute_error: 32.6962\n",
            "\n",
            "Epoch 00003: val_loss improved from 33.26434 to 32.69624, saving model to Weights-003--32.69624.hdf5\n",
            "Epoch 4/50\n",
            "633/633 [==============================] - 0s 84us/step - loss: 13.3662 - mean_absolute_error: 13.3662 - val_loss: 33.1132 - val_mean_absolute_error: 33.1132\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 32.69624\n",
            "Epoch 5/50\n",
            "633/633 [==============================] - 0s 97us/step - loss: 13.2189 - mean_absolute_error: 13.2189 - val_loss: 33.3359 - val_mean_absolute_error: 33.3359\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 32.69624\n",
            "Epoch 6/50\n",
            "633/633 [==============================] - 0s 106us/step - loss: 13.0053 - mean_absolute_error: 13.0053 - val_loss: 33.6284 - val_mean_absolute_error: 33.6284\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 32.69624\n",
            "Epoch 7/50\n",
            "633/633 [==============================] - 0s 93us/step - loss: 12.8955 - mean_absolute_error: 12.8955 - val_loss: 32.3153 - val_mean_absolute_error: 32.3153\n",
            "\n",
            "Epoch 00007: val_loss improved from 32.69624 to 32.31533, saving model to Weights-007--32.31533.hdf5\n",
            "Epoch 8/50\n",
            "633/633 [==============================] - 0s 102us/step - loss: 12.8136 - mean_absolute_error: 12.8136 - val_loss: 32.8763 - val_mean_absolute_error: 32.8763\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 32.31533\n",
            "Epoch 9/50\n",
            "633/633 [==============================] - 0s 88us/step - loss: 12.5681 - mean_absolute_error: 12.5681 - val_loss: 32.0975 - val_mean_absolute_error: 32.0975\n",
            "\n",
            "Epoch 00009: val_loss improved from 32.31533 to 32.09746, saving model to Weights-009--32.09746.hdf5\n",
            "Epoch 10/50\n",
            "633/633 [==============================] - 0s 87us/step - loss: 12.4635 - mean_absolute_error: 12.4635 - val_loss: 32.3539 - val_mean_absolute_error: 32.3539\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 32.09746\n",
            "Epoch 11/50\n",
            "633/633 [==============================] - 0s 88us/step - loss: 12.4947 - mean_absolute_error: 12.4947 - val_loss: 31.2945 - val_mean_absolute_error: 31.2945\n",
            "\n",
            "Epoch 00011: val_loss improved from 32.09746 to 31.29453, saving model to Weights-011--31.29453.hdf5\n",
            "Epoch 12/50\n",
            "633/633 [==============================] - 0s 87us/step - loss: 12.2129 - mean_absolute_error: 12.2129 - val_loss: 31.8089 - val_mean_absolute_error: 31.8089\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 31.29453\n",
            "Epoch 13/50\n",
            "633/633 [==============================] - 0s 89us/step - loss: 11.9849 - mean_absolute_error: 11.9849 - val_loss: 31.4535 - val_mean_absolute_error: 31.4535\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 31.29453\n",
            "Epoch 14/50\n",
            "633/633 [==============================] - 0s 97us/step - loss: 11.8566 - mean_absolute_error: 11.8566 - val_loss: 31.3673 - val_mean_absolute_error: 31.3673\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 31.29453\n",
            "Epoch 15/50\n",
            "633/633 [==============================] - 0s 94us/step - loss: 11.7950 - mean_absolute_error: 11.7950 - val_loss: 31.3556 - val_mean_absolute_error: 31.3556\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 31.29453\n",
            "Epoch 16/50\n",
            "633/633 [==============================] - 0s 89us/step - loss: 11.7058 - mean_absolute_error: 11.7058 - val_loss: 29.8741 - val_mean_absolute_error: 29.8741\n",
            "\n",
            "Epoch 00016: val_loss improved from 31.29453 to 29.87410, saving model to Weights-016--29.87410.hdf5\n",
            "Epoch 17/50\n",
            "633/633 [==============================] - 0s 93us/step - loss: 11.5868 - mean_absolute_error: 11.5868 - val_loss: 31.8118 - val_mean_absolute_error: 31.8118\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 29.87410\n",
            "Epoch 18/50\n",
            "633/633 [==============================] - 0s 90us/step - loss: 11.7744 - mean_absolute_error: 11.7744 - val_loss: 31.8360 - val_mean_absolute_error: 31.8360\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 29.87410\n",
            "Epoch 19/50\n",
            "633/633 [==============================] - 0s 96us/step - loss: 11.5734 - mean_absolute_error: 11.5734 - val_loss: 31.2087 - val_mean_absolute_error: 31.2087\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 29.87410\n",
            "Epoch 20/50\n",
            "633/633 [==============================] - 0s 92us/step - loss: 11.6215 - mean_absolute_error: 11.6215 - val_loss: 29.1653 - val_mean_absolute_error: 29.1653\n",
            "\n",
            "Epoch 00020: val_loss improved from 29.87410 to 29.16526, saving model to Weights-020--29.16526.hdf5\n",
            "Epoch 21/50\n",
            "633/633 [==============================] - 0s 103us/step - loss: 11.7604 - mean_absolute_error: 11.7604 - val_loss: 30.6568 - val_mean_absolute_error: 30.6568\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 29.16526\n",
            "Epoch 22/50\n",
            "633/633 [==============================] - 0s 88us/step - loss: 11.4660 - mean_absolute_error: 11.4660 - val_loss: 29.9685 - val_mean_absolute_error: 29.9685\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 29.16526\n",
            "Epoch 23/50\n",
            "633/633 [==============================] - 0s 88us/step - loss: 11.2542 - mean_absolute_error: 11.2542 - val_loss: 29.9102 - val_mean_absolute_error: 29.9102\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 29.16526\n",
            "Epoch 24/50\n",
            "633/633 [==============================] - 0s 103us/step - loss: 11.3073 - mean_absolute_error: 11.3073 - val_loss: 29.3041 - val_mean_absolute_error: 29.3041\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 29.16526\n",
            "Epoch 25/50\n",
            "633/633 [==============================] - 0s 109us/step - loss: 11.3768 - mean_absolute_error: 11.3768 - val_loss: 30.0640 - val_mean_absolute_error: 30.0640\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 29.16526\n",
            "Epoch 26/50\n",
            "633/633 [==============================] - 0s 93us/step - loss: 11.1841 - mean_absolute_error: 11.1841 - val_loss: 31.5931 - val_mean_absolute_error: 31.5931\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 29.16526\n",
            "Epoch 27/50\n",
            "633/633 [==============================] - 0s 96us/step - loss: 11.2717 - mean_absolute_error: 11.2717 - val_loss: 31.3105 - val_mean_absolute_error: 31.3105\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 29.16526\n",
            "Epoch 28/50\n",
            "633/633 [==============================] - 0s 90us/step - loss: 11.3530 - mean_absolute_error: 11.3530 - val_loss: 31.6708 - val_mean_absolute_error: 31.6708\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 29.16526\n",
            "Epoch 29/50\n",
            "633/633 [==============================] - 0s 93us/step - loss: 10.8523 - mean_absolute_error: 10.8523 - val_loss: 31.2736 - val_mean_absolute_error: 31.2736\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 29.16526\n",
            "Epoch 30/50\n",
            "633/633 [==============================] - 0s 89us/step - loss: 10.7922 - mean_absolute_error: 10.7922 - val_loss: 30.9954 - val_mean_absolute_error: 30.9954\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 29.16526\n",
            "Epoch 31/50\n",
            "633/633 [==============================] - 0s 89us/step - loss: 10.7403 - mean_absolute_error: 10.7403 - val_loss: 32.2695 - val_mean_absolute_error: 32.2695\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 29.16526\n",
            "Epoch 32/50\n",
            "633/633 [==============================] - 0s 88us/step - loss: 11.0017 - mean_absolute_error: 11.0017 - val_loss: 32.4724 - val_mean_absolute_error: 32.4724\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 29.16526\n",
            "Epoch 33/50\n",
            "633/633 [==============================] - 0s 87us/step - loss: 10.8220 - mean_absolute_error: 10.8220 - val_loss: 31.6477 - val_mean_absolute_error: 31.6477\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 29.16526\n",
            "Epoch 34/50\n",
            "633/633 [==============================] - 0s 86us/step - loss: 10.6932 - mean_absolute_error: 10.6932 - val_loss: 32.6339 - val_mean_absolute_error: 32.6339\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 29.16526\n",
            "Epoch 35/50\n",
            "633/633 [==============================] - 0s 90us/step - loss: 11.1453 - mean_absolute_error: 11.1453 - val_loss: 30.1476 - val_mean_absolute_error: 30.1476\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 29.16526\n",
            "Epoch 36/50\n",
            "633/633 [==============================] - 0s 90us/step - loss: 10.5850 - mean_absolute_error: 10.5850 - val_loss: 31.5035 - val_mean_absolute_error: 31.5035\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 29.16526\n",
            "Epoch 37/50\n",
            "633/633 [==============================] - 0s 94us/step - loss: 10.5290 - mean_absolute_error: 10.5290 - val_loss: 31.4937 - val_mean_absolute_error: 31.4937\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 29.16526\n",
            "Epoch 38/50\n",
            "633/633 [==============================] - 0s 94us/step - loss: 10.5419 - mean_absolute_error: 10.5419 - val_loss: 31.2522 - val_mean_absolute_error: 31.2522\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 29.16526\n",
            "Epoch 39/50\n",
            "633/633 [==============================] - 0s 88us/step - loss: 10.5932 - mean_absolute_error: 10.5932 - val_loss: 32.2668 - val_mean_absolute_error: 32.2668\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 29.16526\n",
            "Epoch 40/50\n",
            "633/633 [==============================] - 0s 97us/step - loss: 10.3961 - mean_absolute_error: 10.3961 - val_loss: 30.4237 - val_mean_absolute_error: 30.4237\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 29.16526\n",
            "Epoch 41/50\n",
            "633/633 [==============================] - 0s 91us/step - loss: 10.5991 - mean_absolute_error: 10.5991 - val_loss: 30.0965 - val_mean_absolute_error: 30.0965\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 29.16526\n",
            "Epoch 42/50\n",
            "633/633 [==============================] - 0s 91us/step - loss: 10.3498 - mean_absolute_error: 10.3498 - val_loss: 32.6647 - val_mean_absolute_error: 32.6647\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 29.16526\n",
            "Epoch 43/50\n",
            "633/633 [==============================] - 0s 91us/step - loss: 10.7624 - mean_absolute_error: 10.7624 - val_loss: 31.8200 - val_mean_absolute_error: 31.8200\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 29.16526\n",
            "Epoch 44/50\n",
            "633/633 [==============================] - 0s 89us/step - loss: 10.4292 - mean_absolute_error: 10.4292 - val_loss: 31.4004 - val_mean_absolute_error: 31.4004\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 29.16526\n",
            "Epoch 45/50\n",
            "633/633 [==============================] - 0s 89us/step - loss: 10.3359 - mean_absolute_error: 10.3359 - val_loss: 32.4466 - val_mean_absolute_error: 32.4466\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 29.16526\n",
            "Epoch 46/50\n",
            "633/633 [==============================] - 0s 88us/step - loss: 10.4527 - mean_absolute_error: 10.4527 - val_loss: 29.1434 - val_mean_absolute_error: 29.1434\n",
            "\n",
            "Epoch 00046: val_loss improved from 29.16526 to 29.14336, saving model to Weights-046--29.14336.hdf5\n",
            "Epoch 47/50\n",
            "633/633 [==============================] - 0s 91us/step - loss: 10.9489 - mean_absolute_error: 10.9489 - val_loss: 30.3744 - val_mean_absolute_error: 30.3744\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 29.14336\n",
            "Epoch 48/50\n",
            "633/633 [==============================] - 0s 92us/step - loss: 10.4447 - mean_absolute_error: 10.4447 - val_loss: 32.0649 - val_mean_absolute_error: 32.0649\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 29.14336\n",
            "Epoch 49/50\n",
            "633/633 [==============================] - 0s 89us/step - loss: 10.5331 - mean_absolute_error: 10.5331 - val_loss: 32.3604 - val_mean_absolute_error: 32.3604\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 29.14336\n",
            "Epoch 50/50\n",
            "633/633 [==============================] - 0s 91us/step - loss: 10.4261 - mean_absolute_error: 10.4261 - val_loss: 31.5652 - val_mean_absolute_error: 31.5652\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 29.14336\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f728a1f3630>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkfoVe17nB5F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load weights file of the best model\n",
        "# weights_file = 'Weights-036--29.68963.hdf5' # choose the best checkpoint \n",
        "weights_file = '/content/drive/Shared drives/COS485 project/Weights/Weights-042--29.79151.hdf5'\n",
        "NN_model_2.load_weights(weights_file) # load it\n",
        "NN_model_2.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uvb4RNAnsVzd",
        "colab_type": "text"
      },
      "source": [
        "The validation loss is 29.14336"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rFWe3w1XoKGP"
      },
      "source": [
        "#Network 3: 5 Layers, Sigmoid and Relu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-KcmYnGroKGT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "outputId": "a5b602c0-bb89-47d9-9a8b-6e57adbfceb4"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error \n",
        "\n",
        "NN_model_5 = Sequential()\n",
        "\n",
        "#Input Layer:\n",
        "NN_model_5.add(Dense(128, kernel_initializer='normal', input_dim = train.shape[1], activation='relu'))\n",
        "\n",
        "#Five Hidden Layers:\n",
        "NN_model_5.add(Dense(256, kernel_initializer='normal', activation='sigmoid'))\n",
        "NN_model_5.add(Dense(256, kernel_initializer='normal', activation='relu'))\n",
        "NN_model_5.add(Dense(256, kernel_initializer='normal', activation='sigmoid'))\n",
        "NN_model_5.add(Dense(256, kernel_initializer='normal', activation='sigmoid'))\n",
        "NN_model_5.add(Dense(256, kernel_initializer='normal', activation='relu'))\n",
        "\n",
        "\n",
        "# The output layer:\n",
        "NN_model_5.add(Dense(1, kernel_initializer='normal', activation='linear'))\n",
        "\n",
        "# Compile the network :\n",
        "NN_model_5.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
        "NN_model_5.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_8 (Dense)              (None, 128)               768       \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 256)               33024     \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 297,217\n",
            "Trainable params: 297,217\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Yq_WeEIzoKGt",
        "colab": {}
      },
      "source": [
        "checkpoint_name = 'Weights-{epoch:07d}--{val_loss:.5f}.hdf5' \n",
        "checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KBLSGYPCoKHF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a123a440-3569-41c4-98ce-189a21d0dfef"
      },
      "source": [
        "NN_model_5.fit(train, train_target, epochs=50, batch_size=32, validation_split = 0.2, callbacks=callbacks_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 633 samples, validate on 159 samples\n",
            "Epoch 1/50\n",
            "633/633 [==============================] - 0s 427us/step - loss: 14.0276 - mean_absolute_error: 14.0276 - val_loss: 34.3133 - val_mean_absolute_error: 34.3133\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 34.31327, saving model to Weights-001--34.31327.hdf5\n",
            "Epoch 2/50\n",
            "633/633 [==============================] - 0s 174us/step - loss: 13.9797 - mean_absolute_error: 13.9797 - val_loss: 33.9303 - val_mean_absolute_error: 33.9303\n",
            "\n",
            "Epoch 00002: val_loss improved from 34.31327 to 33.93026, saving model to Weights-002--33.93026.hdf5\n",
            "Epoch 3/50\n",
            "633/633 [==============================] - 0s 177us/step - loss: 13.8597 - mean_absolute_error: 13.8597 - val_loss: 33.2891 - val_mean_absolute_error: 33.2891\n",
            "\n",
            "Epoch 00003: val_loss improved from 33.93026 to 33.28907, saving model to Weights-003--33.28907.hdf5\n",
            "Epoch 4/50\n",
            "633/633 [==============================] - 0s 179us/step - loss: 13.5754 - mean_absolute_error: 13.5754 - val_loss: 31.8107 - val_mean_absolute_error: 31.8107\n",
            "\n",
            "Epoch 00004: val_loss improved from 33.28907 to 31.81075, saving model to Weights-004--31.81075.hdf5\n",
            "Epoch 5/50\n",
            "633/633 [==============================] - 0s 182us/step - loss: 13.5386 - mean_absolute_error: 13.5386 - val_loss: 33.1556 - val_mean_absolute_error: 33.1556\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 31.81075\n",
            "Epoch 6/50\n",
            "633/633 [==============================] - 0s 176us/step - loss: 13.0682 - mean_absolute_error: 13.0682 - val_loss: 33.8367 - val_mean_absolute_error: 33.8367\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 31.81075\n",
            "Epoch 7/50\n",
            "633/633 [==============================] - 0s 181us/step - loss: 13.0083 - mean_absolute_error: 13.0083 - val_loss: 31.4748 - val_mean_absolute_error: 31.4748\n",
            "\n",
            "Epoch 00007: val_loss improved from 31.81075 to 31.47480, saving model to Weights-007--31.47480.hdf5\n",
            "Epoch 8/50\n",
            "633/633 [==============================] - 0s 178us/step - loss: 13.2558 - mean_absolute_error: 13.2558 - val_loss: 32.3401 - val_mean_absolute_error: 32.3401\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 31.47480\n",
            "Epoch 9/50\n",
            "633/633 [==============================] - 0s 172us/step - loss: 12.9821 - mean_absolute_error: 12.9821 - val_loss: 34.2834 - val_mean_absolute_error: 34.2834\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 31.47480\n",
            "Epoch 10/50\n",
            "633/633 [==============================] - 0s 191us/step - loss: 12.8721 - mean_absolute_error: 12.8721 - val_loss: 31.7432 - val_mean_absolute_error: 31.7432\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 31.47480\n",
            "Epoch 11/50\n",
            "633/633 [==============================] - 0s 170us/step - loss: 12.6481 - mean_absolute_error: 12.6481 - val_loss: 33.1522 - val_mean_absolute_error: 33.1522\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 31.47480\n",
            "Epoch 12/50\n",
            "633/633 [==============================] - 0s 175us/step - loss: 12.5333 - mean_absolute_error: 12.5333 - val_loss: 33.8363 - val_mean_absolute_error: 33.8363\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 31.47480\n",
            "Epoch 13/50\n",
            "633/633 [==============================] - 0s 176us/step - loss: 12.5341 - mean_absolute_error: 12.5341 - val_loss: 32.1151 - val_mean_absolute_error: 32.1151\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 31.47480\n",
            "Epoch 14/50\n",
            "633/633 [==============================] - 0s 186us/step - loss: 12.3200 - mean_absolute_error: 12.3200 - val_loss: 30.8905 - val_mean_absolute_error: 30.8905\n",
            "\n",
            "Epoch 00014: val_loss improved from 31.47480 to 30.89050, saving model to Weights-014--30.89050.hdf5\n",
            "Epoch 15/50\n",
            "633/633 [==============================] - 0s 180us/step - loss: 12.3230 - mean_absolute_error: 12.3230 - val_loss: 31.8958 - val_mean_absolute_error: 31.8958\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 30.89050\n",
            "Epoch 16/50\n",
            "633/633 [==============================] - 0s 179us/step - loss: 12.2000 - mean_absolute_error: 12.2000 - val_loss: 31.5628 - val_mean_absolute_error: 31.5628\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 30.89050\n",
            "Epoch 17/50\n",
            "633/633 [==============================] - 0s 183us/step - loss: 12.0168 - mean_absolute_error: 12.0168 - val_loss: 31.4393 - val_mean_absolute_error: 31.4393\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 30.89050\n",
            "Epoch 18/50\n",
            "633/633 [==============================] - 0s 181us/step - loss: 11.9778 - mean_absolute_error: 11.9778 - val_loss: 31.3124 - val_mean_absolute_error: 31.3124\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 30.89050\n",
            "Epoch 19/50\n",
            "633/633 [==============================] - 0s 172us/step - loss: 11.6974 - mean_absolute_error: 11.6974 - val_loss: 32.6365 - val_mean_absolute_error: 32.6365\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 30.89050\n",
            "Epoch 20/50\n",
            "633/633 [==============================] - 0s 175us/step - loss: 11.7803 - mean_absolute_error: 11.7802 - val_loss: 33.0247 - val_mean_absolute_error: 33.0247\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 30.89050\n",
            "Epoch 21/50\n",
            "633/633 [==============================] - 0s 169us/step - loss: 12.1713 - mean_absolute_error: 12.1713 - val_loss: 29.6720 - val_mean_absolute_error: 29.6721\n",
            "\n",
            "Epoch 00021: val_loss improved from 30.89050 to 29.67205, saving model to Weights-021--29.67205.hdf5\n",
            "Epoch 22/50\n",
            "633/633 [==============================] - 0s 183us/step - loss: 12.2001 - mean_absolute_error: 12.2001 - val_loss: 31.9214 - val_mean_absolute_error: 31.9214\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 29.67205\n",
            "Epoch 23/50\n",
            "633/633 [==============================] - 0s 179us/step - loss: 12.0993 - mean_absolute_error: 12.0993 - val_loss: 28.7316 - val_mean_absolute_error: 28.7316\n",
            "\n",
            "Epoch 00023: val_loss improved from 29.67205 to 28.73163, saving model to Weights-023--28.73163.hdf5\n",
            "Epoch 24/50\n",
            "633/633 [==============================] - 0s 191us/step - loss: 12.5385 - mean_absolute_error: 12.5385 - val_loss: 31.1063 - val_mean_absolute_error: 31.1063\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 28.73163\n",
            "Epoch 25/50\n",
            "633/633 [==============================] - 0s 174us/step - loss: 11.4396 - mean_absolute_error: 11.4396 - val_loss: 31.2198 - val_mean_absolute_error: 31.2198\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 28.73163\n",
            "Epoch 26/50\n",
            "633/633 [==============================] - 0s 180us/step - loss: 11.2281 - mean_absolute_error: 11.2281 - val_loss: 29.5880 - val_mean_absolute_error: 29.5880\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 28.73163\n",
            "Epoch 27/50\n",
            "633/633 [==============================] - 0s 171us/step - loss: 10.9793 - mean_absolute_error: 10.9793 - val_loss: 33.9970 - val_mean_absolute_error: 33.9970\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 28.73163\n",
            "Epoch 28/50\n",
            "633/633 [==============================] - 0s 170us/step - loss: 11.8762 - mean_absolute_error: 11.8762 - val_loss: 29.9937 - val_mean_absolute_error: 29.9937\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 28.73163\n",
            "Epoch 29/50\n",
            "633/633 [==============================] - 0s 169us/step - loss: 10.8598 - mean_absolute_error: 10.8598 - val_loss: 34.2391 - val_mean_absolute_error: 34.2391\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 28.73163\n",
            "Epoch 30/50\n",
            "633/633 [==============================] - 0s 167us/step - loss: 12.8351 - mean_absolute_error: 12.8351 - val_loss: 30.9293 - val_mean_absolute_error: 30.9293\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 28.73163\n",
            "Epoch 31/50\n",
            "633/633 [==============================] - 0s 180us/step - loss: 11.3402 - mean_absolute_error: 11.3402 - val_loss: 32.4913 - val_mean_absolute_error: 32.4913\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 28.73163\n",
            "Epoch 32/50\n",
            "633/633 [==============================] - 0s 178us/step - loss: 11.0645 - mean_absolute_error: 11.0645 - val_loss: 29.9421 - val_mean_absolute_error: 29.9421\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 28.73163\n",
            "Epoch 33/50\n",
            "633/633 [==============================] - 0s 169us/step - loss: 10.8891 - mean_absolute_error: 10.8891 - val_loss: 32.5225 - val_mean_absolute_error: 32.5225\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 28.73163\n",
            "Epoch 34/50\n",
            "633/633 [==============================] - 0s 174us/step - loss: 10.6197 - mean_absolute_error: 10.6197 - val_loss: 31.2524 - val_mean_absolute_error: 31.2524\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 28.73163\n",
            "Epoch 35/50\n",
            "633/633 [==============================] - 0s 175us/step - loss: 10.6321 - mean_absolute_error: 10.6321 - val_loss: 33.3271 - val_mean_absolute_error: 33.3271\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 28.73163\n",
            "Epoch 36/50\n",
            "633/633 [==============================] - 0s 171us/step - loss: 10.8369 - mean_absolute_error: 10.8369 - val_loss: 30.8519 - val_mean_absolute_error: 30.8519\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 28.73163\n",
            "Epoch 37/50\n",
            "633/633 [==============================] - 0s 171us/step - loss: 10.5527 - mean_absolute_error: 10.5527 - val_loss: 31.6584 - val_mean_absolute_error: 31.6584\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 28.73163\n",
            "Epoch 38/50\n",
            "633/633 [==============================] - 0s 170us/step - loss: 12.0619 - mean_absolute_error: 12.0619 - val_loss: 35.1687 - val_mean_absolute_error: 35.1687\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 28.73163\n",
            "Epoch 39/50\n",
            "633/633 [==============================] - 0s 178us/step - loss: 13.9571 - mean_absolute_error: 13.9571 - val_loss: 33.7474 - val_mean_absolute_error: 33.7474\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 28.73163\n",
            "Epoch 40/50\n",
            "633/633 [==============================] - 0s 179us/step - loss: 13.3471 - mean_absolute_error: 13.3471 - val_loss: 32.2255 - val_mean_absolute_error: 32.2255\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 28.73163\n",
            "Epoch 41/50\n",
            "633/633 [==============================] - 0s 172us/step - loss: 12.8495 - mean_absolute_error: 12.8495 - val_loss: 31.4408 - val_mean_absolute_error: 31.4408\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 28.73163\n",
            "Epoch 42/50\n",
            "633/633 [==============================] - 0s 176us/step - loss: 12.4431 - mean_absolute_error: 12.4431 - val_loss: 31.0020 - val_mean_absolute_error: 31.0020\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 28.73163\n",
            "Epoch 43/50\n",
            "633/633 [==============================] - 0s 171us/step - loss: 11.3734 - mean_absolute_error: 11.3734 - val_loss: 32.4968 - val_mean_absolute_error: 32.4968\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 28.73163\n",
            "Epoch 44/50\n",
            "633/633 [==============================] - 0s 174us/step - loss: 12.1788 - mean_absolute_error: 12.1788 - val_loss: 33.9148 - val_mean_absolute_error: 33.9148\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 28.73163\n",
            "Epoch 45/50\n",
            "633/633 [==============================] - 0s 172us/step - loss: 11.7279 - mean_absolute_error: 11.7279 - val_loss: 32.9175 - val_mean_absolute_error: 32.9175\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 28.73163\n",
            "Epoch 46/50\n",
            "633/633 [==============================] - 0s 178us/step - loss: 10.9061 - mean_absolute_error: 10.9061 - val_loss: 32.3014 - val_mean_absolute_error: 32.3014\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 28.73163\n",
            "Epoch 47/50\n",
            "633/633 [==============================] - 0s 169us/step - loss: 10.8227 - mean_absolute_error: 10.8227 - val_loss: 32.7837 - val_mean_absolute_error: 32.7837\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 28.73163\n",
            "Epoch 48/50\n",
            "633/633 [==============================] - 0s 170us/step - loss: 10.9568 - mean_absolute_error: 10.9568 - val_loss: 33.3457 - val_mean_absolute_error: 33.3457\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 28.73163\n",
            "Epoch 49/50\n",
            "633/633 [==============================] - 0s 179us/step - loss: 10.6153 - mean_absolute_error: 10.6153 - val_loss: 32.8929 - val_mean_absolute_error: 32.8929\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 28.73163\n",
            "Epoch 50/50\n",
            "633/633 [==============================] - 0s 190us/step - loss: 10.7404 - mean_absolute_error: 10.7404 - val_loss: 31.2470 - val_mean_absolute_error: 31.2470\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 28.73163\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f72892f1cc0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fkng6xQmoKHW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "outputId": "ade848f3-b7a1-4293-c609-1645ddb08b6f"
      },
      "source": [
        "# Load weights file of the best model\n",
        "# weights_file = 'Weights-036--29.68963.hdf5' # choose the best checkpoint \n",
        "weights_file = '/content/drive/Shared drives/COS485 project/Weights/Weights-036--29.68963.hdf5'\n",
        "NN_model_5.load_weights(weights_file) # load it\n",
        "NN_model_5.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-36f5c234304d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# weights_file = 'Weights-036--29.68963.hdf5' # choose the best checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mweights_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/Shared drives/COS485 project/Weights/Weights-036--29.68963.hdf5'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mNN_model_5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# load it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mNN_model_5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean_absolute_error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mean_absolute_error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_filepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mload_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/network.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, reshape)\u001b[0m\n\u001b[1;32m   1228\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1229\u001b[0m                 saving.load_weights_from_hdf5_group(\n\u001b[0;32m-> 1230\u001b[0;31m                     f, self.layers, reshape=reshape)\n\u001b[0m\u001b[1;32m   1231\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'close'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1232\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[0;34m(f, layers, reshape)\u001b[0m\n\u001b[1;32m   1207\u001b[0m                          \u001b[0;34m'containing '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1208\u001b[0m                          \u001b[0;34m' layers into a model with '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1209\u001b[0;31m                          str(len(filtered_layers)) + ' layers.')\n\u001b[0m\u001b[1;32m   1210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1211\u001b[0m     \u001b[0;31m# We batch weight value assignments in a single backend call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: You are trying to load a weight file containing 3 layers into a model with 7 layers."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXe9sLfco8bj",
        "colab_type": "text"
      },
      "source": [
        "The validation loss is 28.73163"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gBQF12SUpDJm"
      },
      "source": [
        "#Network 4: 10 Layers, Sigmoid and ReLu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Kl8_WFEBpDJs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 580
        },
        "outputId": "f40c0567-0e30-4392-8e4b-1dfcade94218"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error \n",
        "\n",
        "NN_model_10 = Sequential()\n",
        "\n",
        "#Input Layer:\n",
        "NN_model_10.add(Dense(128, kernel_initializer='normal', input_dim = train.shape[1], activation='relu'))\n",
        "\n",
        "#Ten Hidden Layers:\n",
        "NN_model_10.add(Dense(256, kernel_initializer='normal', activation='sigmoid'))\n",
        "NN_model_10.add(Dense(256, kernel_initializer='normal', activation='relu'))\n",
        "NN_model_10.add(Dense(256, kernel_initializer='normal', activation='sigmoid'))\n",
        "NN_model_10.add(Dense(256, kernel_initializer='normal', activation='relu'))\n",
        "NN_model_10.add(Dense(256, kernel_initializer='normal', activation='sigmoid'))\n",
        "NN_model_10.add(Dense(256, kernel_initializer='normal', activation='relu'))\n",
        "NN_model_10.add(Dense(256, kernel_initializer='normal', activation='sigmoid'))\n",
        "NN_model_10.add(Dense(256, kernel_initializer='normal', activation='relu'))\n",
        "NN_model_10.add(Dense(256, kernel_initializer='normal', activation='sigmoid'))\n",
        "NN_model_10.add(Dense(256, kernel_initializer='normal', activation='relu'))\n",
        "\n",
        "\n",
        "# The output layer:\n",
        "NN_model_10.add(Dense(1, kernel_initializer='normal', activation='linear'))\n",
        "\n",
        "# Compile the network :\n",
        "NN_model_10.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
        "NN_model_10.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_15 (Dense)             (None, 128)               768       \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 256)               33024     \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dense_24 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dense_25 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dense_26 (Dense)             (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 626,177\n",
            "Trainable params: 626,177\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MhxM85mDpDJ6",
        "colab": {}
      },
      "source": [
        "checkpoint_name = 'Weights-{epoch:12d}--{val_loss:.5f}.hdf5' \n",
        "checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jWQ4oOp2pDKC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e09fcec7-35ae-494d-ddcb-dd67979baac8"
      },
      "source": [
        "NN_model_10.fit(train, train_target, epochs=50, batch_size=32, validation_split = 0.2, callbacks=callbacks_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 633 samples, validate on 159 samples\n",
            "Epoch 1/50\n",
            "633/633 [==============================] - 0s 746us/step - loss: 14.0012 - mean_absolute_error: 14.0012 - val_loss: 34.4675 - val_mean_absolute_error: 34.4675\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 34.46754, saving model to Weights-001--34.46754.hdf5\n",
            "Epoch 2/50\n",
            "633/633 [==============================] - 0s 325us/step - loss: 13.9933 - mean_absolute_error: 13.9933 - val_loss: 34.1750 - val_mean_absolute_error: 34.1750\n",
            "\n",
            "Epoch 00002: val_loss improved from 34.46754 to 34.17497, saving model to Weights-002--34.17497.hdf5\n",
            "Epoch 3/50\n",
            "633/633 [==============================] - 0s 333us/step - loss: 13.9806 - mean_absolute_error: 13.9806 - val_loss: 34.2125 - val_mean_absolute_error: 34.2125\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 34.17497\n",
            "Epoch 4/50\n",
            "633/633 [==============================] - 0s 321us/step - loss: 13.9810 - mean_absolute_error: 13.9810 - val_loss: 34.1753 - val_mean_absolute_error: 34.1753\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 34.17497\n",
            "Epoch 5/50\n",
            "633/633 [==============================] - 0s 333us/step - loss: 13.9672 - mean_absolute_error: 13.9672 - val_loss: 33.9651 - val_mean_absolute_error: 33.9651\n",
            "\n",
            "Epoch 00005: val_loss improved from 34.17497 to 33.96508, saving model to Weights-005--33.96508.hdf5\n",
            "Epoch 6/50\n",
            "633/633 [==============================] - 0s 305us/step - loss: 13.9544 - mean_absolute_error: 13.9544 - val_loss: 34.0467 - val_mean_absolute_error: 34.0467\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 33.96508\n",
            "Epoch 7/50\n",
            "633/633 [==============================] - 0s 340us/step - loss: 13.9571 - mean_absolute_error: 13.9571 - val_loss: 34.0175 - val_mean_absolute_error: 34.0175\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 33.96508\n",
            "Epoch 8/50\n",
            "633/633 [==============================] - 0s 326us/step - loss: 13.9594 - mean_absolute_error: 13.9594 - val_loss: 34.4656 - val_mean_absolute_error: 34.4656\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 33.96508\n",
            "Epoch 9/50\n",
            "633/633 [==============================] - 0s 319us/step - loss: 14.0103 - mean_absolute_error: 14.0103 - val_loss: 34.2100 - val_mean_absolute_error: 34.2100\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 33.96508\n",
            "Epoch 10/50\n",
            "633/633 [==============================] - 0s 306us/step - loss: 13.9762 - mean_absolute_error: 13.9762 - val_loss: 34.1952 - val_mean_absolute_error: 34.1952\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 33.96508\n",
            "Epoch 11/50\n",
            "633/633 [==============================] - 0s 325us/step - loss: 13.9832 - mean_absolute_error: 13.9832 - val_loss: 34.2426 - val_mean_absolute_error: 34.2425\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 33.96508\n",
            "Epoch 12/50\n",
            "633/633 [==============================] - 0s 325us/step - loss: 13.8645 - mean_absolute_error: 13.8645 - val_loss: 33.1066 - val_mean_absolute_error: 33.1066\n",
            "\n",
            "Epoch 00012: val_loss improved from 33.96508 to 33.10664, saving model to Weights-012--33.10664.hdf5\n",
            "Epoch 13/50\n",
            "633/633 [==============================] - 0s 349us/step - loss: 13.6868 - mean_absolute_error: 13.6868 - val_loss: 32.5796 - val_mean_absolute_error: 32.5796\n",
            "\n",
            "Epoch 00013: val_loss improved from 33.10664 to 32.57963, saving model to Weights-013--32.57963.hdf5\n",
            "Epoch 14/50\n",
            "633/633 [==============================] - 0s 311us/step - loss: 13.7199 - mean_absolute_error: 13.7199 - val_loss: 35.0185 - val_mean_absolute_error: 35.0185\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 32.57963\n",
            "Epoch 15/50\n",
            "633/633 [==============================] - 0s 317us/step - loss: 13.7303 - mean_absolute_error: 13.7303 - val_loss: 32.4749 - val_mean_absolute_error: 32.4749\n",
            "\n",
            "Epoch 00015: val_loss improved from 32.57963 to 32.47493, saving model to Weights-015--32.47493.hdf5\n",
            "Epoch 16/50\n",
            "633/633 [==============================] - 0s 321us/step - loss: 13.6529 - mean_absolute_error: 13.6529 - val_loss: 33.0526 - val_mean_absolute_error: 33.0526\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 32.47493\n",
            "Epoch 17/50\n",
            "633/633 [==============================] - 0s 339us/step - loss: 13.5579 - mean_absolute_error: 13.5579 - val_loss: 33.4288 - val_mean_absolute_error: 33.4288\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 32.47493\n",
            "Epoch 18/50\n",
            "633/633 [==============================] - 0s 329us/step - loss: 13.8299 - mean_absolute_error: 13.8299 - val_loss: 34.2997 - val_mean_absolute_error: 34.2997\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 32.47493\n",
            "Epoch 19/50\n",
            "633/633 [==============================] - 0s 340us/step - loss: 13.9929 - mean_absolute_error: 13.9929 - val_loss: 34.0825 - val_mean_absolute_error: 34.0825\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 32.47493\n",
            "Epoch 20/50\n",
            "633/633 [==============================] - 0s 316us/step - loss: 13.9530 - mean_absolute_error: 13.9530 - val_loss: 33.9473 - val_mean_absolute_error: 33.9473\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 32.47493\n",
            "Epoch 21/50\n",
            "633/633 [==============================] - 0s 324us/step - loss: 13.9575 - mean_absolute_error: 13.9575 - val_loss: 34.0667 - val_mean_absolute_error: 34.0667\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 32.47493\n",
            "Epoch 22/50\n",
            "633/633 [==============================] - 0s 335us/step - loss: 13.9588 - mean_absolute_error: 13.9588 - val_loss: 34.0179 - val_mean_absolute_error: 34.0179\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 32.47493\n",
            "Epoch 23/50\n",
            "633/633 [==============================] - 0s 329us/step - loss: 13.9532 - mean_absolute_error: 13.9532 - val_loss: 34.0096 - val_mean_absolute_error: 34.0096\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 32.47493\n",
            "Epoch 24/50\n",
            "633/633 [==============================] - 0s 327us/step - loss: 13.9578 - mean_absolute_error: 13.9578 - val_loss: 34.0016 - val_mean_absolute_error: 34.0016\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 32.47493\n",
            "Epoch 25/50\n",
            "633/633 [==============================] - 0s 320us/step - loss: 13.9525 - mean_absolute_error: 13.9525 - val_loss: 34.0120 - val_mean_absolute_error: 34.0120\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 32.47493\n",
            "Epoch 26/50\n",
            "633/633 [==============================] - 0s 322us/step - loss: 13.9537 - mean_absolute_error: 13.9537 - val_loss: 33.9991 - val_mean_absolute_error: 33.9991\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 32.47493\n",
            "Epoch 27/50\n",
            "633/633 [==============================] - 0s 350us/step - loss: 13.9507 - mean_absolute_error: 13.9507 - val_loss: 33.9850 - val_mean_absolute_error: 33.9850\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 32.47493\n",
            "Epoch 28/50\n",
            "633/633 [==============================] - 0s 310us/step - loss: 13.9516 - mean_absolute_error: 13.9516 - val_loss: 33.9894 - val_mean_absolute_error: 33.9894\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 32.47493\n",
            "Epoch 29/50\n",
            "633/633 [==============================] - 0s 314us/step - loss: 13.9555 - mean_absolute_error: 13.9555 - val_loss: 34.0379 - val_mean_absolute_error: 34.0379\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 32.47493\n",
            "Epoch 30/50\n",
            "633/633 [==============================] - 0s 302us/step - loss: 13.9580 - mean_absolute_error: 13.9580 - val_loss: 34.0680 - val_mean_absolute_error: 34.0680\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 32.47493\n",
            "Epoch 31/50\n",
            "633/633 [==============================] - 0s 314us/step - loss: 13.9569 - mean_absolute_error: 13.9569 - val_loss: 34.0058 - val_mean_absolute_error: 34.0058\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 32.47493\n",
            "Epoch 32/50\n",
            "633/633 [==============================] - 0s 322us/step - loss: 13.9543 - mean_absolute_error: 13.9543 - val_loss: 33.9967 - val_mean_absolute_error: 33.9967\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 32.47493\n",
            "Epoch 33/50\n",
            "633/633 [==============================] - 0s 317us/step - loss: 13.9564 - mean_absolute_error: 13.9564 - val_loss: 34.0433 - val_mean_absolute_error: 34.0434\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 32.47493\n",
            "Epoch 34/50\n",
            "633/633 [==============================] - 0s 302us/step - loss: 13.9543 - mean_absolute_error: 13.9543 - val_loss: 34.0222 - val_mean_absolute_error: 34.0222\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 32.47493\n",
            "Epoch 35/50\n",
            "633/633 [==============================] - 0s 313us/step - loss: 13.9524 - mean_absolute_error: 13.9524 - val_loss: 33.9986 - val_mean_absolute_error: 33.9986\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 32.47493\n",
            "Epoch 36/50\n",
            "633/633 [==============================] - 0s 312us/step - loss: 13.9536 - mean_absolute_error: 13.9536 - val_loss: 33.9936 - val_mean_absolute_error: 33.9936\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 32.47493\n",
            "Epoch 37/50\n",
            "633/633 [==============================] - 0s 379us/step - loss: 13.9495 - mean_absolute_error: 13.9495 - val_loss: 34.0566 - val_mean_absolute_error: 34.0566\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 32.47493\n",
            "Epoch 38/50\n",
            "633/633 [==============================] - 0s 312us/step - loss: 13.9592 - mean_absolute_error: 13.9592 - val_loss: 33.9905 - val_mean_absolute_error: 33.9905\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 32.47493\n",
            "Epoch 39/50\n",
            "633/633 [==============================] - 0s 327us/step - loss: 13.9497 - mean_absolute_error: 13.9497 - val_loss: 34.0429 - val_mean_absolute_error: 34.0429\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 32.47493\n",
            "Epoch 40/50\n",
            "633/633 [==============================] - 0s 320us/step - loss: 13.9573 - mean_absolute_error: 13.9573 - val_loss: 33.9917 - val_mean_absolute_error: 33.9917\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 32.47493\n",
            "Epoch 41/50\n",
            "633/633 [==============================] - 0s 329us/step - loss: 13.9591 - mean_absolute_error: 13.9591 - val_loss: 34.0004 - val_mean_absolute_error: 34.0004\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 32.47493\n",
            "Epoch 42/50\n",
            "633/633 [==============================] - 0s 327us/step - loss: 13.9536 - mean_absolute_error: 13.9536 - val_loss: 34.0003 - val_mean_absolute_error: 34.0003\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 32.47493\n",
            "Epoch 43/50\n",
            "633/633 [==============================] - 0s 324us/step - loss: 13.9604 - mean_absolute_error: 13.9604 - val_loss: 34.0459 - val_mean_absolute_error: 34.0459\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 32.47493\n",
            "Epoch 44/50\n",
            "633/633 [==============================] - 0s 320us/step - loss: 13.9566 - mean_absolute_error: 13.9566 - val_loss: 34.0415 - val_mean_absolute_error: 34.0415\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 32.47493\n",
            "Epoch 45/50\n",
            "633/633 [==============================] - 0s 318us/step - loss: 13.9549 - mean_absolute_error: 13.9549 - val_loss: 34.0575 - val_mean_absolute_error: 34.0575\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 32.47493\n",
            "Epoch 46/50\n",
            "633/633 [==============================] - 0s 312us/step - loss: 13.9553 - mean_absolute_error: 13.9553 - val_loss: 33.9655 - val_mean_absolute_error: 33.9655\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 32.47493\n",
            "Epoch 47/50\n",
            "633/633 [==============================] - 0s 335us/step - loss: 13.9586 - mean_absolute_error: 13.9586 - val_loss: 34.1077 - val_mean_absolute_error: 34.1077\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 32.47493\n",
            "Epoch 48/50\n",
            "633/633 [==============================] - 0s 318us/step - loss: 13.9572 - mean_absolute_error: 13.9572 - val_loss: 34.0267 - val_mean_absolute_error: 34.0267\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 32.47493\n",
            "Epoch 49/50\n",
            "633/633 [==============================] - 0s 316us/step - loss: 13.9571 - mean_absolute_error: 13.9571 - val_loss: 34.0699 - val_mean_absolute_error: 34.0699\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 32.47493\n",
            "Epoch 50/50\n",
            "633/633 [==============================] - 0s 317us/step - loss: 13.9566 - mean_absolute_error: 13.9566 - val_loss: 33.9895 - val_mean_absolute_error: 33.9895\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 32.47493\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f72893531d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTWp0YrTsENn",
        "colab_type": "text"
      },
      "source": [
        "The validation loss is 32.47493"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6GTG4qswpDKO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "outputId": "6b1df34a-1d77-4d08-ea7e-066b59656072"
      },
      "source": [
        "# Load weights file of the best model\n",
        "# weights_file = 'Weights-036--29.68963.hdf5' # choose the best checkpoint \n",
        "weights_file = '/content/drive/Shared drives/COS485 project/Weights/Weights-036--29.68963.hdf5'\n",
        "NN_model_10.load_weights(weights_file) # load it\n",
        "NN_model_10.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-3e04b401a1fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# weights_file = 'Weights-036--29.68963.hdf5' # choose the best checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mweights_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/Shared drives/COS485 project/Weights/Weights-036--29.68963.hdf5'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mNN_model_10\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# load it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mNN_model_10\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean_absolute_error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mean_absolute_error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_filepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mload_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/network.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, reshape)\u001b[0m\n\u001b[1;32m   1228\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1229\u001b[0m                 saving.load_weights_from_hdf5_group(\n\u001b[0;32m-> 1230\u001b[0;31m                     f, self.layers, reshape=reshape)\n\u001b[0m\u001b[1;32m   1231\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'close'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1232\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[0;34m(f, layers, reshape)\u001b[0m\n\u001b[1;32m   1207\u001b[0m                          \u001b[0;34m'containing '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1208\u001b[0m                          \u001b[0;34m' layers into a model with '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1209\u001b[0;31m                          str(len(filtered_layers)) + ' layers.')\n\u001b[0m\u001b[1;32m   1210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1211\u001b[0m     \u001b[0;31m# We batch weight value assignments in a single backend call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: You are trying to load a weight file containing 3 layers into a model with 12 layers."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNqADZ3qHJTb",
        "colab_type": "text"
      },
      "source": [
        "#Misc/ NOT IN USE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5Uenieu3888",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seasons = [] # a matrix of \"weeks\" matrices \n",
        "# weeks = [] # a matrix of Week objects - there will be many of these\n",
        "\n",
        "class Week: # an object holding 3 features\n",
        "  def __init__(self, weekNumber, cases, temperature):\n",
        "    self.weekNumber = weekNumber\n",
        "    self.cases = cases\n",
        "    self.temperature = temperature\n",
        "\n",
        "# take in data and populate seasons[] with weeks[] matrices\n",
        "# and weeks[] matrices with Week objects\n",
        "\n",
        "numberOfSeasons = len(seasons)\n",
        "# seasonLength = len(weeks)\n",
        "for i in range(1, numberOfSeasons) # so we start at  1, not 0\n",
        "  # should be consistent across the line anyway, but just in case\n",
        "  seasonLength = len(seasons[i]) \n",
        "  for j in range(1, seasonLength) # CDC numbers the weeks starting at 1\n",
        "    weekNumber = j\n",
        "    cases = \n",
        "    temperature = \n",
        "    w = Week(weekNumber, cases, temperature)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4LECHMDnnDR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# fn to visualize training \n",
        "def visualize_training(errsq, errcl, x0, x1, x2, W1, t):\n",
        "    # convert things to numpy, numpy is easier to use with matplotlib \n",
        "    errsq, errcl, x0, x1, x2, W1 = [x.numpy() for x in [errsq, errcl, x0, x1, x2, W1]]\n",
        "    \n",
        "    # visualization\n",
        "    avgerrsq = np.divide( np.cumsum(errsq[0:t]), np.arange(1,t+1) )\n",
        "    avgerrcl = np.divide( np.cumsum(errcl[0:t]), np.arange(1,t+1) )\n",
        "\n",
        "    fig = plt.figure(figsize=fig_size)\n",
        "    gs = gridspec.GridSpec(6,5, wspace=0.3, hspace=0.1)\n",
        "\n",
        "    ax = fig.add_subplot(gs[0,0])\n",
        "    ax.axis('off')\n",
        "    ax.set_title(\"x0 at t={}\".format(t))\n",
        "    ax.imshow(x0.reshape(28,28), origin='upper', interpolation='nearest') \n",
        "\n",
        "    ax = fig.add_subplot(gs[0,1])\n",
        "    ax.bar(range(len(x1)), x1)\n",
        "    ax.set_ylabel(\"layer1\")\n",
        "    ax.set_ylim((-1, 2))\n",
        "    ax.grid()\n",
        "\n",
        "    ax = fig.add_subplot(gs[0,2])\n",
        "    ax.bar(range(len(x2)), x2)\n",
        "    ax.set_ylabel(\"output\")\n",
        "    ax.set_ylim((-1,2))\n",
        "    ax.set_xticks(range(0,10))\n",
        "    ax.grid()\n",
        "\n",
        "    ax = fig.add_subplot(gs[0,3:5])\n",
        "    ax.plot(avgerrsq, label=\"squared\")\n",
        "    ax.plot(avgerrcl, label=\"class\")\n",
        "    ax.set_ylabel(\"error\")\n",
        "    ax.set_ylim([0,1])\n",
        "    ax.grid()\n",
        "    ax.legend()\n",
        "\n",
        "    n1 = W1.shape[0]\n",
        "    for i in range(n1):\n",
        "        ax = fig.add_subplot(gs[1 + i // 5, i % 5])\n",
        "        ax.axis('off')\n",
        "        ax.imshow(W1.T.reshape(28,28,n1)[:,:,i], origin='upper', interpolation='nearest') \n",
        "\n",
        "    display.display(plt.gcf())\n",
        "    time.sleep(0.01)\n",
        "    display.clear_output(wait=True)\n",
        "    fig.clf()\n",
        "    plt.close(fig)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CW4px43nnDT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# functions to build and train networks \n",
        "def train_twolayer_network(f, df, eta, epsinit, tmax, fig_size):\n",
        "    \"\"\"Build and train two layer neural network\n",
        "    \n",
        "    Args:\n",
        "        f: function, activation function, maps torch tensor to another torch tensor\n",
        "        df: function, derivative of activation function, maps torch tensor to another torch tensor\n",
        "        eta: float, learning rate\n",
        "        epsinit: float, magnitude of initial weights\n",
        "        tmax: int, max number of training updates\n",
        "        fig_size: (x, y) size of figure to plot, may need to change to properly fit your screen\n",
        "    \"\"\"\n",
        "    ### setup network architecture ###\n",
        "    m = 60000                # number of examples in training set\n",
        "\n",
        "    n0 = 784                 # widths of layers\n",
        "    n1 = 25\n",
        "    n2 = 10  \n",
        "\n",
        "    # two fully connected synaptic layers\n",
        "    W1 = epsinit*torch.randn(n1, n0)\n",
        "    W2 = epsinit*torch.randn(n2, n1)\n",
        "\n",
        "    # biases\n",
        "    b1 = epsinit*torch.randn(n1)\n",
        "    b2 = epsinit*torch.randn(n2)\n",
        "\n",
        "    # training parameters\n",
        "    tshow = 1000             # how often to pause for visualization\n",
        "    errsq = torch.zeros(tmax) # track squared error \n",
        "    errcl = torch.zeros(tmax) # track classification error \n",
        "\n",
        "    ### iterate through training data ###\n",
        "    for t in range(tmax):\n",
        "        # get and format example\n",
        "        i = int(np.floor(m*np.random.rand()))\n",
        "        x0 = get_features(i)\n",
        "        y = get_onehot(i)\n",
        "\n",
        "        # forward pass\n",
        "        x1 = f(torch.matmul(W1,x0) + b1)\n",
        "        x2 = f(torch.matmul(W2,x1) + b2)\n",
        "\n",
        "        # compute error \n",
        "        errsq[t] = 0.5*torch.sum((y-x2)**2)\n",
        "        errcl[t] = float(np.argmax(x2.numpy()) != trainlabels[i])\n",
        "\n",
        "        # backward pass\n",
        "        delta2 = -1*(y-x2) * df(x2)\n",
        "        delta1 = torch.matmul(torch.t(W2), delta2) * df(x1)\n",
        "\n",
        "        # update weights\n",
        "        W2 -= eta * torch.ger(delta2, x1) # torch.ger(x,y) means outer product of x,y\n",
        "        W1 -= eta * torch.ger(delta1, x0)\n",
        "        b2 -= eta * delta2\n",
        "        b1 -= eta * delta1\n",
        "\n",
        "        # visualize training \n",
        "        if t % tshow == 0: \n",
        "            visualize_training(errsq, errcl, x0, x1, x2, W1, t)\n",
        "            \n",
        "def train_threelayer_network(f, df, eta, epsinit, tmax, fig_size): \n",
        "    ### Your code here ###\n",
        "    raise NotImplementedError"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1h8GRo6nnDW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Train 2-layer perceptron with sigmoid activation ###\n",
        "def f(x):\n",
        "    return 1 / (1+torch.exp(-x))\n",
        "def df(y):\n",
        "    return y * (1-y)\n",
        "eta = 0.1 # learning rate \n",
        "epsinit = 0.01 #initial scale of weights\n",
        "tmax = int(6*10**5) # number of training iterations \n",
        "fig_size = (20,20) # size of figure, may need to modify to make plots fit screen\n",
        "\n",
        "train_twolayer_network(f, df, eta, epsinit, tmax, fig_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Gz-g3d-IZiJ",
        "colab_type": "text"
      },
      "source": [
        "**Autograd**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvcOEMumDtGF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# autograd version of two layer network\n",
        "def train_twolayer_network_autograd(f, eta, epsinit, tmax, fig_size):\n",
        "    \"\"\"Build and train two layer neural network\n",
        "    \n",
        "    Args:\n",
        "        f: function, activation function, maps torch tensor to another torch tensor\n",
        "        eta: float, learning rate\n",
        "        epsinit: float, magnitude of initial weights\n",
        "        tmax: int, max number of training updates\n",
        "        fig_size: (x, y) size of figure to plot, may need to change to properly fit your screen\n",
        "    \"\"\"\n",
        "    ### setup network architecture ###\n",
        "    m = 60000                # number of examples in training set\n",
        "\n",
        "    n0 = 784                 # widths of layers\n",
        "    n1 = 25\n",
        "    n2 = 10  \n",
        "\n",
        "    # two fully connected synaptic layers\n",
        "    W1 = nn.Parameter(epsinit*torch.randn(n1, n0))\n",
        "    W2 = nn.Parameter(epsinit*torch.randn(n2, n1))\n",
        "\n",
        "    # biases\n",
        "    b1 = nn.Parameter(epsinit*torch.randn(n1))\n",
        "    b2 = nn.Parameter(epsinit*torch.randn(n2))\n",
        "    \n",
        "    # create optimizer\n",
        "    optim = torch.optim.SGD([W1,W2,b1,b2], eta)\n",
        "\n",
        "    # training parameters\n",
        "    tshow = 1000             # how often to pause for visualization\n",
        "    errsq = torch.zeros(tmax) # track squared error \n",
        "    errcl = torch.zeros(tmax) # track classification error \n",
        "\n",
        "    ### iterate through training data ###\n",
        "    for t in range(tmax):\n",
        "        # get and format example\n",
        "        i = int(np.floor(m*np.random.rand()))\n",
        "        x0 = get_features(i)\n",
        "        y = get_onehot(i)\n",
        "        \n",
        "        # zero gradients, without this, pytorch will add new grads to previous ones\n",
        "        optim.zero_grad()\n",
        "\n",
        "        # forward pass\n",
        "        x1 = f(torch.matmul(W1,x0) + b1)\n",
        "        x2 = f(torch.matmul(W2,x1) + b2)\n",
        "\n",
        "        # compute error \n",
        "        e = 0.5*torch.sum((y-x2)**2)\n",
        "        errsq[t] = e\n",
        "        errcl[t] = float(np.argmax(x2.detach().numpy()) != trainlabels[i])\n",
        "\n",
        "        # backward pass\n",
        "        e.backward()\n",
        "\n",
        "        # update weights\n",
        "        optim.step()\n",
        "\n",
        "        # visualize training \n",
        "        if t % tshow == 0: \n",
        "            visualize_training(errsq.detach(), errcl.detach(), x0.detach(), x1.detach(), x2.detach(), W1.detach(), t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2e0b386FRab",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Train 2-layer perceptron with sigmoid activation ###\n",
        "def f(x):\n",
        "    return 1 / (1+torch.exp(-x))\n",
        "\n",
        "eta = 0.1 # learning rate \n",
        "epsinit = 0.01 #initial scale of weights\n",
        "tmax = int(6*10**5) # number of training iterations \n",
        "fig_size = (20,20) # size of figure, may need to modify to make plots fit screen\n",
        "\n",
        "train_twolayer_network_autograd(f, eta, epsinit, tmax, fig_size)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}